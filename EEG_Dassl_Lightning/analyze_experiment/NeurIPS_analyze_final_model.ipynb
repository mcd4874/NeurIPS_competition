{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b25eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from util.util import generate_data_paths,generate_history_results_path, load_history_data, generate_concat_dataset,filter_history_information,load_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0684de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_history(data_table, pick_cols, col_pick_model=\"val_loss\", pick_min=True, max_epochs=100,min_epoch=10,\n",
    "                      col_pick_max=\"test_acc\", data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        fix_col_pick_model = col_pick_model\n",
    "        history_data = pd.read_csv(path)\n",
    "        available_cols = history_data.columns\n",
    "  \n",
    "        # check if col_pick_model exist\n",
    "        if not col_pick_model in history_data.columns:\n",
    "            print(\"col {} isn't in the history data \".format(col_pick_model))\n",
    "            print(\"use default val_loss as pick col\")\n",
    "            fix_col_pick_model = \"val_loss\"\n",
    "        \n",
    "\n",
    "        # limit total epoch to max epoch\n",
    "        history_epoch = len(history_data)\n",
    "        if history_epoch > max_epochs:\n",
    "            history_data = history_data[:max_epochs]\n",
    "        \n",
    "        if history_epoch > min_epoch:\n",
    "            history_data = history_data[min_epoch:]\n",
    "#             print(\"update history data :\",history_data.head())\n",
    "            history_data = history_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # deal with how to use a metric to pick best model\n",
    "        if pick_min:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmin()\n",
    "        else:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmax()\n",
    "\n",
    "        #\n",
    "\n",
    "        # val_loss_name = 'val_loss' if 'val_loss' in history_data.columns else 'val_loss_x'\n",
    "        metric_pick_model = ['']\n",
    "        # get max possible test_auc score information\n",
    "        best_row_idx = history_data[col_pick_max].argmax()\n",
    "\n",
    "#         print(\"pick row idx : \",pick_row_idx)\n",
    "#         print(\"pick best idx : \",best_row_idx)\n",
    "        \n",
    "        # best_col_pick_model = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # best_col_pick_max = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # \n",
    "        # best_test_auc = history_data.loc[best_row_idx, col_pick_max]\n",
    "        test_class_col = [col for col in pick_cols if \"test_class_\" in col]\n",
    "\n",
    "        history_info_dict = {\n",
    "            \"model_choice\": [\"best_possible_epoch\", \"picked_epoch\"],\n",
    "            \"epoch\": [best_row_idx, pick_row_idx],\n",
    "            col_pick_max: [history_data.loc[best_row_idx, col_pick_max], history_data.loc[pick_row_idx, col_pick_max]],\n",
    "            fix_col_pick_model: [history_data.loc[best_row_idx, fix_col_pick_model],\n",
    "                                 history_data.loc[pick_row_idx, fix_col_pick_model]],\n",
    "            \"history_path\": [path, path]\n",
    "        }\n",
    "\n",
    "        history_information = pd.DataFrame(history_info_dict)\n",
    "        history_information_table.append(history_information)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def generate_history_results_path(row, full_result_path):\n",
    "    remain='default\\\\version_0\\\\metrics.csv'\n",
    "    test_fold = row['test_fold']\n",
    "    shuffle_fold = row['shuffle_fold']\n",
    "    increment_fold = row['increment_fold']\n",
    "    valid_fold = row['valid_fold']\n",
    "    history_path = os.path.join(full_result_path,test_fold, shuffle_fold,increment_fold, valid_fold,\n",
    "                                remain)\n",
    "#     print(\"current history path : \",history_path)\n",
    "    return history_path\n",
    "def load_history_data(data_table, pick_cols, data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    print(\"temp col : \",temp.columns)\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        history_data = pd.read_csv(path)\n",
    "        history_data[data_path_col] = [path] * len(history_data)\n",
    "        history_information_table.append(history_data)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def load_data(data_paths, result_folder, result_file_name, info_file_name, info_file_folder=None,load_history=False):\n",
    "    list_data = []\n",
    "    if len(data_paths) ==0:\n",
    "        print(\"no data path \")\n",
    "    for data_path in data_paths:\n",
    "        result_folder_path = os.path.join(data_path, result_folder)\n",
    "        result_data_path = os.path.join(result_folder_path, result_file_name)\n",
    "        if info_file_folder is None: \n",
    "            info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "        else:\n",
    "            info_file_path = os.path.join(data_path, info_file_folder)\n",
    "            info_data_path = os.path.join(info_file_path,info_file_name)\n",
    "\n",
    "        # check if file result exists\n",
    "        if os.path.exists(result_data_path):\n",
    "            data = pd.read_excel(result_data_path)\n",
    "            data_size = len(data)\n",
    "#             info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "            if os.path.exists(info_data_path):\n",
    "                with open(info_data_path) as f:\n",
    "                    info_data = json.load(f)\n",
    "                    extra_fields = info_data[\"EXTRA_FIELDS\"]\n",
    "                    field_names = list(extra_fields.keys())\n",
    "                    for field_name in field_names:\n",
    "                        if extra_fields[field_name] == []:\n",
    "                            extra_fields[field_name] = None\n",
    "                        data[field_name] = data_size*[extra_fields[field_name]]\n",
    "                list_data.append(data)\n",
    "            else:\n",
    "                print(\"no data info for {} \".format(result_data_path))\n",
    "\n",
    "            if load_history:\n",
    "                data['history_path'] = data.apply(lambda row: generate_history_results_path(row, data_path), axis=1)\n",
    "#                 print(\"load current history path : \",data['history_path'].values[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"the current data path {} does not exist \".format(result_data_path))\n",
    "\n",
    "    final_data = pd.concat(list_data).reset_index(drop=True)\n",
    "    return final_data\n",
    "# prefix_lists=[augmentation_prefix,norm_prefix,model_prefix,dataset_prefix]\n",
    "def load_experiment_data(common_path,result_folder = 'result_folder',file_name = 'model_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder=None,prefix_lists=None,pick_cols=None,\n",
    "                         col_pick_model=None,col_pick_model_min=True,\n",
    "                         new_col_generate=None,load_history = False):\n",
    "\n",
    "#     result_folder = 'result_folder'\n",
    "#     file_name = 'model_result.xlsx'\n",
    "#     info_file_name = 'model_info.json'\n",
    "\n",
    "    list_full_path = generate_data_paths(common_path, prefix_lists, [])\n",
    "    data_result = load_data(list_full_path, result_folder, file_name, info_file_name, load_history=load_history,info_file_folder=info_file_folder)\n",
    "    data_cols = data_result.columns\n",
    "\n",
    "    pick_cols = ['test_fold', 'shuffle_fold', 'increment_fold',\n",
    "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
    "       'model', 'source_label_space', 'target_label_space','history_path']\n",
    "    if pick_cols is None:\n",
    "        pick_cols = list(data_cols)\n",
    "\n",
    "#     if new_col_generate is not None:\n",
    "#         for col_generate in new_col_generate:    \n",
    "#             new_col_name = col_generate[0]\n",
    "#             func = col_generate[1]\n",
    "#             data_result[new_col_name] = data_result.apply(lambda row: func(row,data_cols), axis=1)\n",
    "#             pick_cols.append(new_col_name)\n",
    "    if col_pick_model is None:\n",
    "        col_pick_model = 'val_loss'\n",
    "    pick_min = col_pick_model_min\n",
    "    print(\"data result cols : \",data_result.columns)\n",
    "    if load_history:\n",
    "        summary = summarize_history(data_result, pick_cols)\n",
    "        history_data = load_history_data(data_result, pick_cols)\n",
    "        return data_result,history_data,summary\n",
    "    return data_result\n",
    "\n",
    "\n",
    "\n",
    "def modify_col_info(data_result):\n",
    "    cols = list(data_result.columns)\n",
    "    if 'increment_fold' in cols:\n",
    "        data_result['increment_fold'] = data_result['increment_fold'].replace(\n",
    "        ['increment_fold_1', 'increment_fold_2', 'increment_fold_3'], ['1', '2', '3'])\n",
    "    if 'valid_fold' in cols:\n",
    "        data_result['valid_fold'] = data_result['valid_fold'].replace(\n",
    "            ['valid_fold_1', 'valid_fold_2', 'valid_fold_3','valid_fold_4'], ['1', '2', '3','4'])\n",
    "    if 'test_fold' in cols:\n",
    "        data_result['test_fold'] = data_result['test_fold'].replace(\n",
    "            ['test_fold_1','test_fold_2','test_fold_3','test_fold_4','test_fold_5'], ['1','2','3','4','5'])\n",
    "    data_result['aug'] = data_result['aug'].replace(\n",
    "        ['no_aug', 'temporal_aug'], ['no', 'temp'])\n",
    "    data_result['normalize'] = data_result['normalize'].replace(\n",
    "        ['chan_norm', 'no_norm'], ['chan', 'no'])\n",
    "    data_result['model'] = data_result['model'].replace(\n",
    "        ['ComponentAdaptation', 'BaseModel', 'MultiDatasetAdaptation','MultiDatasetAdaptationV1'], ['component', 'base','adapt','adaptV1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b74550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compare \n",
    "# model_list_prefix = [\n",
    "#     'vanilla',\n",
    "#     'adaptation',\n",
    "#     'adaptationV1'\n",
    "# ]\n",
    "# target_dataset_list_prefix = [\n",
    "#     \"dataset_A\",\n",
    "#     \"dataset_B\",\n",
    "# ]\n",
    "# augmentation_list_prefix = [\n",
    "#     'no_aug',\n",
    "#     'temp_aug',\n",
    "#     'T_F_aug'\n",
    "# ]\n",
    "# norm_list_prefix = [\n",
    "#     'no_norm',\n",
    "#     'chan_norm'\n",
    "# ]\n",
    "# prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_6\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# modify_col_info(data_result_1)\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# # table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# # print(table)\n",
    "\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4235080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.456250\n",
      "                              adaptV1  0.532500\n",
      "                              base     0.311250\n",
      "                         temp adapt    0.425000\n",
      "                              adaptV1  0.536250\n",
      "                              base     0.308750\n",
      "               no        no   adapt    0.447500\n",
      "                              adaptV1  0.306250\n",
      "                              base     0.252500\n",
      "                         temp adapt    0.473750\n",
      "                              adaptV1  0.342500\n",
      "                              base     0.405000\n",
      "dataset_B      chan      no   adapt    0.525000\n",
      "                              adaptV1  0.576389\n",
      "                              base     0.354167\n",
      "                         temp adapt    0.561111\n",
      "                              adaptV1  0.593056\n",
      "                              base     0.534375\n",
      "               no        no   adapt    0.554167\n",
      "                              adaptV1  0.538194\n",
      "                              base     0.386458\n",
      "                         temp adapt    0.558333\n",
      "                              adaptV1  0.494444\n",
      "                              base     0.507292\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "    'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "save_data_folder = \"NeurIPS\\data\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3_avg.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68da812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.456250\n",
      "                              adaptV1  0.533750\n",
      "                              base     0.312500\n",
      "                         temp adapt    0.430000\n",
      "                              adaptV1  0.536250\n",
      "                              base     0.310000\n",
      "               no        no   adapt    0.481250\n",
      "                              adaptV1  0.376250\n",
      "                              base     0.337500\n",
      "                         temp adapt    0.422500\n",
      "                              adaptV1  0.318750\n",
      "                              base     0.377500\n",
      "dataset_B      chan      no   adapt    0.524306\n",
      "                              adaptV1  0.577778\n",
      "                              base     0.352083\n",
      "                         temp adapt    0.563194\n",
      "                              adaptV1  0.593750\n",
      "                              base     0.528125\n",
      "               no        no   adapt    0.554861\n",
      "                              adaptV1  0.570833\n",
      "                              base     0.387500\n",
      "                         temp adapt    0.543750\n",
      "                              adaptV1  0.519444\n",
      "                              base     0.506250\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "    'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_4_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "save_data_folder = \"NeurIPS\\data\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3_avg.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95a52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de99457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\no_aug\\no_norm\\adaptation\\dataset_A\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\no_aug\\no_norm\\adaptation\\dataset_B\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\no_aug\\chan_norm\\adaptation\\dataset_A\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\no_aug\\chan_norm\\adaptation\\dataset_B\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\no_aug\\chan_norm\\adaptationV1\\dataset_A\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\no_aug\\chan_norm\\adaptationV1\\dataset_B\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\temp_aug\\no_norm\\adaptation\\dataset_A\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\temp_aug\\no_norm\\adaptation\\dataset_B\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\temp_aug\\chan_norm\\adaptation\\dataset_A\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\temp_aug\\chan_norm\\adaptation\\dataset_B\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\temp_aug\\chan_norm\\adaptationV1\\dataset_A\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_3\\temp_aug\\chan_norm\\adaptationV1\\dataset_B\\model\\predict_folder_old\\ensemble_result.xlsx does not exist \n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      no        no   adaptV1  0.525000\n",
      "                         temp adaptV1  0.500000\n",
      "dataset_B      no        no   adaptV1  0.611111\n",
      "                         temp adaptV1  0.500000\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder_old',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6b29fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.445000\n",
      "                              adaptV1  0.465000\n",
      "                         temp adapt    0.495000\n",
      "                              adaptV1  0.455000\n",
      "               no        no   adapt    0.470000\n",
      "                              adaptV1  0.550000\n",
      "                         temp adapt    0.480000\n",
      "                              adaptV1  0.520000\n",
      "dataset_B      chan      no   adapt    0.622222\n",
      "                              adaptV1  0.700000\n",
      "                         temp adapt    0.613889\n",
      "                              adaptV1  0.683333\n",
      "               no        no   adapt    0.575000\n",
      "                              adaptV1  0.691667\n",
      "                         temp adapt    0.633333\n",
      "                              adaptV1  0.672222\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.525000\n",
      "                              adaptV1  0.425000\n",
      "                         temp adapt    0.500000\n",
      "                              adaptV1  0.400000\n",
      "               no        no   adapt    0.375000\n",
      "                              adaptV1  0.525000\n",
      "                         temp adapt    0.300000\n",
      "                              adaptV1  0.500000\n",
      "dataset_B      chan      no   adapt    0.611111\n",
      "                              adaptV1  0.583333\n",
      "                         temp adapt    0.611111\n",
      "                              adaptV1  0.625000\n",
      "               no        no   adapt    0.597222\n",
      "                              adaptV1  0.611111\n",
      "                         temp adapt    0.555556\n",
      "                              adaptV1  0.500000\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d29dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.500000\n",
      "                         temp adaptV1  0.580000\n",
      "               no        no   adaptV1  0.295000\n",
      "                         temp adaptV1  0.280000\n",
      "dataset_B      chan      no   adaptV1  0.619444\n",
      "                         temp adaptV1  0.608333\n",
      "               no        no   adaptV1  0.500000\n",
      "                         temp adaptV1  0.494444\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "#     'vanilla',\n",
    "#     'adaptation',\n",
    "    'adaptationV1',\n",
    "#     'FBCNET_adaptV1'\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68c89f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.475000\n",
      "                              adaptV1  0.450000\n",
      "                         temp adapt    0.535000\n",
      "                              adaptV1  0.430000\n",
      "               no        no   adapt    0.430000\n",
      "                              adaptV1  0.525000\n",
      "                         temp adapt    0.495000\n",
      "                              adaptV1  0.525000\n",
      "dataset_B      chan      no   adapt    0.638889\n",
      "                              adaptV1  0.688889\n",
      "                         temp adapt    0.619444\n",
      "                              adaptV1  0.713889\n",
      "               no        no   adapt    0.602778\n",
      "                              adaptV1  0.636111\n",
      "                         temp adapt    0.627778\n",
      "                              adaptV1  0.636111\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.500000\n",
      "                              adaptV1  0.500000\n",
      "                         temp adapt    0.575000\n",
      "                              adaptV1  0.400000\n",
      "               no        no   adapt    0.400000\n",
      "                              adaptV1  0.550000\n",
      "                         temp adapt    0.300000\n",
      "                              adaptV1  0.425000\n",
      "dataset_B      chan      no   adapt    0.527778\n",
      "                              adaptV1  0.569444\n",
      "                         temp adapt    0.583333\n",
      "                              adaptV1  0.625000\n",
      "               no        no   adapt    0.583333\n",
      "                              adaptV1  0.527778\n",
      "                         temp adapt    0.597222\n",
      "                              adaptV1  0.458333\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_5\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_5\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f44beb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.438750\n",
      "                         temp adaptV1  0.462500\n",
      "               no        no   adaptV1  0.510000\n",
      "                         temp adaptV1  0.531250\n",
      "dataset_B      chan      no   adaptV1  0.638194\n",
      "                         temp adaptV1  0.604861\n",
      "               no        no   adaptV1  0.631944\n",
      "                         temp adaptV1  0.627778\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.480000\n",
      "                         temp adaptV1  0.505000\n",
      "               no        no   adaptV1  0.540000\n",
      "                         temp adaptV1  0.595000\n",
      "dataset_B      chan      no   adaptV1  0.661111\n",
      "                         temp adaptV1  0.630556\n",
      "               no        no   adaptV1  0.672222\n",
      "                         temp adaptV1  0.644444\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_4_5\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_4_5\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ad875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0ad12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83779e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

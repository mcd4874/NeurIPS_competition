{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b25eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from util.util import generate_data_paths,generate_history_results_path, load_history_data, generate_concat_dataset,filter_history_information,load_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0684de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_history(data_table, pick_cols, col_pick_model=\"val_loss\", pick_min=True, max_epochs=100,min_epoch=10,\n",
    "                      col_pick_max=\"test_acc\", data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        fix_col_pick_model = col_pick_model\n",
    "        history_data = pd.read_csv(path)\n",
    "        available_cols = history_data.columns\n",
    "  \n",
    "        # check if col_pick_model exist\n",
    "        if not col_pick_model in history_data.columns:\n",
    "            print(\"col {} isn't in the history data \".format(col_pick_model))\n",
    "            print(\"use default val_loss as pick col\")\n",
    "            fix_col_pick_model = \"val_loss\"\n",
    "        \n",
    "\n",
    "        # limit total epoch to max epoch\n",
    "        history_epoch = len(history_data)\n",
    "        if history_epoch > max_epochs:\n",
    "            history_data = history_data[:max_epochs]\n",
    "        \n",
    "        if history_epoch > min_epoch:\n",
    "            history_data = history_data[min_epoch:]\n",
    "#             print(\"update history data :\",history_data.head())\n",
    "            history_data = history_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # deal with how to use a metric to pick best model\n",
    "        if pick_min:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmin()\n",
    "        else:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmax()\n",
    "\n",
    "        #\n",
    "\n",
    "        # val_loss_name = 'val_loss' if 'val_loss' in history_data.columns else 'val_loss_x'\n",
    "        metric_pick_model = ['']\n",
    "        # get max possible test_auc score information\n",
    "        best_row_idx = history_data[col_pick_max].argmax()\n",
    "\n",
    "#         print(\"pick row idx : \",pick_row_idx)\n",
    "#         print(\"pick best idx : \",best_row_idx)\n",
    "        \n",
    "        # best_col_pick_model = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # best_col_pick_max = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # \n",
    "        # best_test_auc = history_data.loc[best_row_idx, col_pick_max]\n",
    "        test_class_col = [col for col in pick_cols if \"test_class_\" in col]\n",
    "\n",
    "        history_info_dict = {\n",
    "            \"model_choice\": [\"best_possible_epoch\", \"picked_epoch\"],\n",
    "            \"epoch\": [best_row_idx, pick_row_idx],\n",
    "            col_pick_max: [history_data.loc[best_row_idx, col_pick_max], history_data.loc[pick_row_idx, col_pick_max]],\n",
    "            fix_col_pick_model: [history_data.loc[best_row_idx, fix_col_pick_model],\n",
    "                                 history_data.loc[pick_row_idx, fix_col_pick_model]],\n",
    "            \"history_path\": [path, path]\n",
    "        }\n",
    "\n",
    "        history_information = pd.DataFrame(history_info_dict)\n",
    "        history_information_table.append(history_information)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def generate_history_results_path(row, full_result_path):\n",
    "    remain='default\\\\version_0\\\\metrics.csv'\n",
    "    test_fold = row['test_fold']\n",
    "    shuffle_fold = row['shuffle_fold']\n",
    "    increment_fold = row['increment_fold']\n",
    "    valid_fold = row['valid_fold']\n",
    "    history_path = os.path.join(full_result_path,test_fold, shuffle_fold,increment_fold, valid_fold,\n",
    "                                remain)\n",
    "#     print(\"current history path : \",history_path)\n",
    "    return history_path\n",
    "def load_history_data(data_table, pick_cols, data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    print(\"temp col : \",temp.columns)\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        history_data = pd.read_csv(path)\n",
    "        history_data[data_path_col] = [path] * len(history_data)\n",
    "        history_information_table.append(history_data)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def load_data(data_paths, result_folder, result_file_name, info_file_name, info_file_folder=None,load_history=False):\n",
    "    list_data = []\n",
    "    if len(data_paths) ==0:\n",
    "        print(\"no data path \")\n",
    "    for data_path in data_paths:\n",
    "        result_folder_path = os.path.join(data_path, result_folder)\n",
    "        result_data_path = os.path.join(result_folder_path, result_file_name)\n",
    "        if info_file_folder is None: \n",
    "            info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "        else:\n",
    "            info_file_path = os.path.join(data_path, info_file_folder)\n",
    "            info_data_path = os.path.join(info_file_path,info_file_name)\n",
    "\n",
    "        # check if file result exists\n",
    "        if os.path.exists(result_data_path):\n",
    "            data = pd.read_excel(result_data_path)\n",
    "            data_size = len(data)\n",
    "#             info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "            if os.path.exists(info_data_path):\n",
    "                with open(info_data_path) as f:\n",
    "                    info_data = json.load(f)\n",
    "                    extra_fields = info_data[\"EXTRA_FIELDS\"]\n",
    "                    field_names = list(extra_fields.keys())\n",
    "                    for field_name in field_names:\n",
    "                        if extra_fields[field_name] == []:\n",
    "                            extra_fields[field_name] = None\n",
    "                        data[field_name] = data_size*[extra_fields[field_name]]\n",
    "                list_data.append(data)\n",
    "            else:\n",
    "                print(\"no data info for {} \".format(result_data_path))\n",
    "\n",
    "            if load_history:\n",
    "                data['history_path'] = data.apply(lambda row: generate_history_results_path(row, data_path), axis=1)\n",
    "#                 print(\"load current history path : \",data['history_path'].values[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"the current data path {} does not exist \".format(result_data_path))\n",
    "\n",
    "    final_data = pd.concat(list_data).reset_index(drop=True)\n",
    "    return final_data\n",
    "# prefix_lists=[augmentation_prefix,norm_prefix,model_prefix,dataset_prefix]\n",
    "def load_experiment_data(common_path,result_folder = 'result_folder',file_name = 'model_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder=None,prefix_lists=None,pick_cols=None,\n",
    "                         col_pick_model=None,col_pick_model_min=True,\n",
    "                         new_col_generate=None,load_history = False):\n",
    "\n",
    "#     result_folder = 'result_folder'\n",
    "#     file_name = 'model_result.xlsx'\n",
    "#     info_file_name = 'model_info.json'\n",
    "\n",
    "    list_full_path = generate_data_paths(common_path, prefix_lists, [])\n",
    "    data_result = load_data(list_full_path, result_folder, file_name, info_file_name, load_history=load_history,info_file_folder=info_file_folder)\n",
    "    data_cols = data_result.columns\n",
    "\n",
    "    pick_cols = ['test_fold', 'shuffle_fold', 'increment_fold','shuffle_fold',\n",
    "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
    "       'model', 'source_label_space', 'target_label_space','history_path']\n",
    "    if pick_cols is None:\n",
    "        pick_cols = list(data_cols)\n",
    "\n",
    "#     if new_col_generate is not None:\n",
    "#         for col_generate in new_col_generate:    \n",
    "#             new_col_name = col_generate[0]\n",
    "#             func = col_generate[1]\n",
    "#             data_result[new_col_name] = data_result.apply(lambda row: func(row,data_cols), axis=1)\n",
    "#             pick_cols.append(new_col_name)\n",
    "    if col_pick_model is None:\n",
    "        col_pick_model = 'val_loss'\n",
    "    pick_min = col_pick_model_min\n",
    "    print(\"data result cols : \",data_result.columns)\n",
    "    if load_history:\n",
    "        summary = summarize_history(data_result, pick_cols)\n",
    "        history_data = load_history_data(data_result, pick_cols)\n",
    "        return data_result,history_data,summary\n",
    "    return data_result\n",
    "\n",
    "\n",
    "\n",
    "def modify_col_info(data_result):\n",
    "    cols = list(data_result.columns)\n",
    "    if 'increment_fold' in cols:\n",
    "        data_result['increment_fold'] = data_result['increment_fold'].replace(\n",
    "        ['increment_fold_1', 'increment_fold_2', 'increment_fold_3'], ['1', '2', '3'])\n",
    "    if 'valid_fold' in cols:\n",
    "        data_result['valid_fold'] = data_result['valid_fold'].replace(\n",
    "            ['valid_fold_1', 'valid_fold_2', 'valid_fold_3','valid_fold_4'], ['1', '2', '3','4'])\n",
    "    if 'test_fold' in cols:\n",
    "        data_result['test_fold'] = data_result['test_fold'].replace(\n",
    "            ['test_fold_1','test_fold_2','test_fold_3','test_fold_4','test_fold_5'], ['1','2','3','4','5'])\n",
    "    data_result['aug'] = data_result['aug'].replace(\n",
    "        ['no_aug', 'temporal_aug'], ['no', 'temp'])\n",
    "    data_result['normalize'] = data_result['normalize'].replace(\n",
    "        ['chan_norm', 'no_norm'], ['chan', 'no'])\n",
    "    data_result['model'] = data_result['model'].replace(\n",
    "        ['ComponentAdaptation', 'BaseModel', 'MultiDatasetAdaptation','MultiDatasetAdaptationV1','MultiDatasetDannV1','MultiDatasetMCDV1','MultiDatasetADDAV1'], ['component', 'base','adapt','adaptV1','dannV1','mcdV1','addaV1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b74550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compare \n",
    "# model_list_prefix = [\n",
    "#     'vanilla',\n",
    "#     'adaptation',\n",
    "#     'adaptationV1'\n",
    "# ]\n",
    "# target_dataset_list_prefix = [\n",
    "#     \"dataset_A\",\n",
    "#     \"dataset_B\",\n",
    "# ]\n",
    "# augmentation_list_prefix = [\n",
    "#     'no_aug',\n",
    "#     'temp_aug',\n",
    "#     'T_F_aug'\n",
    "# ]\n",
    "# norm_list_prefix = [\n",
    "#     'no_norm',\n",
    "#     'chan_norm'\n",
    "# ]\n",
    "# prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_6\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# modify_col_info(data_result_1)\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# # table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# # print(table)\n",
    "\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4235080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.456250\n",
      "                              adaptV1  0.532500\n",
      "                              base     0.311250\n",
      "                         temp adapt    0.425000\n",
      "                              adaptV1  0.536250\n",
      "                              base     0.308750\n",
      "               no        no   adapt    0.447500\n",
      "                              adaptV1  0.306250\n",
      "                              base     0.252500\n",
      "                         temp adapt    0.473750\n",
      "                              adaptV1  0.342500\n",
      "                              base     0.405000\n",
      "dataset_B      chan      no   adapt    0.525000\n",
      "                              adaptV1  0.576389\n",
      "                              base     0.354167\n",
      "                         temp adapt    0.561111\n",
      "                              adaptV1  0.593056\n",
      "                              base     0.534375\n",
      "               no        no   adapt    0.554167\n",
      "                              adaptV1  0.538194\n",
      "                              base     0.386458\n",
      "                         temp adapt    0.558333\n",
      "                              adaptV1  0.494444\n",
      "                              base     0.507292\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "    'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "save_data_folder = \"NeurIPS\\data\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3_avg.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a68da812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4\\temp_aug\\no_norm\\adaptationV1\\dataset_A\\model\\result_folder\\model_result.xlsx does not exist \n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_B      chan      no   adaptV1  0.622222\n",
      "                         temp adaptV1  0.608333\n",
      "               no        no   adaptV1  0.358333\n",
      "                         temp adaptV1  0.438889\n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4\\temp_aug\\no_norm\\adaptationV1\\dataset_A\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'backbone', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_B      chan      no   adaptV1  0.541667\n",
      "                         temp adaptV1  0.486111\n",
      "               no        no   adaptV1  0.347222\n",
      "                         temp adaptV1  0.569444\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "#     'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "    'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "save_data_folder = \"NeurIPS\\data\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3_avg.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d95a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\no_aug\\chan_norm\\adaptation\\dataset_A\\model\\result_folder\\model_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\no_aug\\chan_norm\\adaptation\\dataset_B\\model\\result_folder\\model_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\temp_aug\\chan_norm\\adaptation\\dataset_A\\model\\result_folder\\model_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\temp_aug\\chan_norm\\adaptation\\dataset_B\\model\\result_folder\\model_result.xlsx does not exist \n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.500000\n",
      "                         temp adaptV1  0.580000\n",
      "               no        no   adaptV1  0.295000\n",
      "                         temp adaptV1  0.280000\n",
      "dataset_B      chan      no   adaptV1  0.619444\n",
      "                         temp adaptV1  0.608333\n",
      "               no        no   adaptV1  0.500000\n",
      "                         temp adaptV1  0.494444\n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\no_aug\\no_norm\\adaptationV1\\dataset_A\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\no_aug\\no_norm\\adaptationV1\\dataset_B\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\no_aug\\chan_norm\\adaptation\\dataset_A\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\no_aug\\chan_norm\\adaptation\\dataset_B\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\temp_aug\\no_norm\\adaptationV1\\dataset_A\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\temp_aug\\no_norm\\adaptationV1\\dataset_B\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\temp_aug\\chan_norm\\adaptation\\dataset_A\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4_tmp\\temp_aug\\chan_norm\\adaptation\\dataset_B\\model\\predict_folder\\ensemble_result.xlsx does not exist \n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.325000\n",
      "                         temp adaptV1  0.475000\n",
      "dataset_B      chan      no   adaptV1  0.541667\n",
      "                         temp adaptV1  0.486111\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "#     'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "    'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_tmp\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "save_data_folder = \"NeurIPS\\data\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "# print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3_avg.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_tmp\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de99457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.465000\n",
      "                         temp adaptV1  0.455000\n",
      "               no        no   adaptV1  0.525000\n",
      "                         temp adaptV1  0.520000\n",
      "dataset_B      chan      no   adaptV1  0.702778\n",
      "                         temp adaptV1  0.705556\n",
      "               no        no   adaptV1  0.691667\n",
      "                         temp adaptV1  0.680556\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.375000\n",
      "                         temp adaptV1  0.400000\n",
      "               no        no   adaptV1  0.550000\n",
      "                         temp adaptV1  0.475000\n",
      "dataset_B      chan      no   adaptV1  0.597222\n",
      "                         temp adaptV1  0.625000\n",
      "               no        no   adaptV1  0.611111\n",
      "                         temp adaptV1  0.527778\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b29fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.465000\n",
      "                         temp adaptV1  0.455000\n",
      "               no        no   adaptV1  0.550000\n",
      "                         temp adaptV1  0.520000\n",
      "dataset_B      chan      no   adaptV1  0.700000\n",
      "                         temp adaptV1  0.683333\n",
      "               no        no   adaptV1  0.691667\n",
      "                         temp adaptV1  0.672222\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.425000\n",
      "                         temp adaptV1  0.400000\n",
      "               no        no   adaptV1  0.525000\n",
      "                         temp adaptV1  0.500000\n",
      "dataset_B      chan      no   adaptV1  0.583333\n",
      "                         temp adaptV1  0.625000\n",
      "               no        no   adaptV1  0.611111\n",
      "                         temp adaptV1  0.500000\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "#     'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3_tmp\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3_tmp\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d29dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.465000\n",
      "                         temp adaptV1  0.455000\n",
      "               no        no   adaptV1  0.550000\n",
      "                         temp adaptV1  0.520000\n",
      "dataset_B      chan      no   adaptV1  0.700000\n",
      "                         temp adaptV1  0.683333\n",
      "               no        no   adaptV1  0.691667\n",
      "                         temp adaptV1  0.672222\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.425000\n",
      "                         temp adaptV1  0.400000\n",
      "               no        no   adaptV1  0.525000\n",
      "                         temp adaptV1  0.500000\n",
      "dataset_B      chan      no   adaptV1  0.583333\n",
      "                         temp adaptV1  0.625000\n",
      "               no        no   adaptV1  0.611111\n",
      "                         temp adaptV1  0.500000\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "#     'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "#     'FBCNET_adaptV1'\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c89f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.475000\n",
      "                              dannV1   0.495000\n",
      "                         temp adaptV1  0.475000\n",
      "                              dannV1   0.485000\n",
      "               no        no   adaptV1  0.505000\n",
      "                              dannV1   0.580000\n",
      "                         temp adaptV1  0.550000\n",
      "                              dannV1   0.580000\n",
      "dataset_B      chan      no   adaptV1  0.669444\n",
      "                              dannV1   0.722222\n",
      "                         temp adaptV1  0.702778\n",
      "                              dannV1   0.650000\n",
      "               no        no   adaptV1  0.733333\n",
      "                              dannV1   0.700000\n",
      "                         temp adaptV1  0.736111\n",
      "                              dannV1   0.738889\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'backbone', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.475000\n",
      "                              dannV1   0.425000\n",
      "                         temp adaptV1  0.350000\n",
      "                              dannV1   0.375000\n",
      "               no        no   adaptV1  0.400000\n",
      "                              dannV1   0.550000\n",
      "                         temp adaptV1  0.500000\n",
      "                              dannV1   0.475000\n",
      "dataset_B      chan      no   adaptV1  0.583333\n",
      "                              dannV1   0.555556\n",
      "                         temp adaptV1  0.527778\n",
      "                              dannV1   0.500000\n",
      "               no        no   adaptV1  0.597222\n",
      "                              dannV1   0.583333\n",
      "                         temp adaptV1  0.625000\n",
      "                              dannV1   0.625000\n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'aug', 'model',\n",
      "       'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.615000\n",
      "                         temp adaptV1  0.590000\n",
      "               no        no   adaptV1  0.540000\n",
      "                         temp adaptV1  0.565000\n",
      "dataset_B      chan      no   adaptV1  0.691667\n",
      "                         temp adaptV1  0.716667\n",
      "               no        no   adaptV1  0.730556\n",
      "                         temp adaptV1  0.711111\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'aug', 'model', 'backbone', 'source_label_space',\n",
      "       'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.475000\n",
      "                         temp adaptV1  0.525000\n",
      "               no        no   adaptV1  0.500000\n",
      "                         temp adaptV1  0.575000\n",
      "dataset_B      chan      no   adaptV1  0.541667\n",
      "                         temp adaptV1  0.541667\n",
      "               no        no   adaptV1  0.597222\n",
      "                         temp adaptV1  0.555556\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "#     'vanilla',\n",
    "#     'adaptation',\n",
    "    'adaptationV1',\n",
    "    'dannV1'\n",
    "#     'shallowcon_adaptV1'\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_7_1_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_7_1_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "#compare \n",
    "model_list_prefix = [\n",
    "#     'vanilla',\n",
    "#     'adaptation',\n",
    "#     'adaptationV1',\n",
    "    'shallowcon_adaptV1'\n",
    "]\n",
    "##### case 2\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_7_1_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_7_1_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44beb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'LA', 'aug',\n",
      "       'model', 'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.630000\n",
      "                         temp adaptV1  0.630000\n",
      "               no        no   adaptV1  0.600000\n",
      "                         temp adaptV1  0.565000\n",
      "dataset_B      chan      no   adaptV1  0.727778\n",
      "                         temp adaptV1  0.736111\n",
      "               no        no   adaptV1  0.777778\n",
      "                         temp adaptV1  0.763889\n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'LA', 'aug',\n",
      "       'model', 'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.568750\n",
      "                         temp adaptV1  0.601250\n",
      "               no        no   adaptV1  0.631250\n",
      "                         temp adaptV1  0.618750\n",
      "dataset_B      chan      no   adaptV1  0.695833\n",
      "                         temp adaptV1  0.725000\n",
      "               no        no   adaptV1  0.730208\n",
      "                         temp adaptV1  0.721875\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'LA', 'aug', 'model', 'backbone',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.595000\n",
      "                         temp adaptV1  0.625000\n",
      "               no        no   adaptV1  0.650000\n",
      "                         temp adaptV1  0.675000\n",
      "dataset_B      chan      no   adaptV1  0.720833\n",
      "                         temp adaptV1  0.758333\n",
      "               no        no   adaptV1  0.754167\n",
      "                         temp adaptV1  0.750000\n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'LA', 'aug',\n",
      "       'model', 'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.710000\n",
      "                         temp adaptV1  0.650000\n",
      "               no        no   adaptV1  0.695000\n",
      "                         temp adaptV1  0.685000\n",
      "dataset_B      chan      no   adaptV1  0.730556\n",
      "                         temp adaptV1  0.705556\n",
      "               no        no   adaptV1  0.763889\n",
      "                         temp adaptV1  0.727778\n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'LA', 'aug',\n",
      "       'model', 'backbone', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.636250\n",
      "                         temp adaptV1  0.615000\n",
      "               no        no   adaptV1  0.698750\n",
      "                         temp adaptV1  0.690000\n",
      "dataset_B      chan      no   adaptV1  0.607292\n",
      "                         temp adaptV1  0.622917\n",
      "               no        no   adaptV1  0.550000\n",
      "                         temp adaptV1  0.573958\n",
      "data result cols :  Index(['test_fold', 'test_acc', 'target_dataset', 'source_dataset',\n",
      "       'normalize', 'EA', 'LA', 'aug', 'model', 'backbone',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.655000\n",
      "                         temp adaptV1  0.670000\n",
      "               no        no   adaptV1  0.765000\n",
      "                         temp adaptV1  0.720000\n",
      "dataset_B      chan      no   adaptV1  0.633333\n",
      "                         temp adaptV1  0.645833\n",
      "               no        no   adaptV1  0.570833\n",
      "                         temp adaptV1  0.575000\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "    'dannV1'\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_9_0_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\"\"\"experiment 9-0-3\"\"\"\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_9_0_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_9_0_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\"\"\" final result 9-0-1\"\"\"\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_9_0_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_9_0_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_9_0_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "                         info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "872ad875",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'LA', 'aug',\n",
      "       'model', 'backbone', 'source_label_space', 'target_label_space',\n",
      "       'class_0_acc', 'class_1_acc', 'class_2_acc', 'class_3_acc'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       class_0_acc  class_1_acc  class_2_acc  \\\n",
      "target_dataset normalize aug  model                                            \n",
      "dataset_A      chan      no   adaptV1     0.353333     0.553660     0.342222   \n",
      "                              addaV1      0.485476     0.260654     0.477778   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.499048     0.458170     0.558889   \n",
      "                              mcdV1       0.357381     0.301601     0.436667   \n",
      "                         temp adaptV1     0.183333     0.346046     0.507222   \n",
      "                              addaV1      0.387143     0.406144     0.625000   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.187857     0.574020     0.594444   \n",
      "                              mcdV1       0.225476     0.462157     0.471111   \n",
      "               no        no   adaptV1     0.302857     0.540850     0.393333   \n",
      "                              addaV1      0.384524     0.400033     0.399444   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.319286     0.348562     0.541667   \n",
      "                              mcdV1       0.397143     0.348072     0.493333   \n",
      "                         temp adaptV1     0.178333     0.510033     0.414444   \n",
      "                              addaV1      0.369286     0.552353     0.575000   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.351190     0.517549     0.471667   \n",
      "                              mcdV1       0.375238     0.388660     0.444444   \n",
      "dataset_B      chan      no   adaptV1     0.444265     0.396879     0.858114   \n",
      "                              addaV1      0.507888     0.560034     0.779216   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.463971     0.462631     0.837281   \n",
      "                              mcdV1       0.565000     0.401144     0.770921   \n",
      "                         temp adaptV1     0.427794     0.415588     0.912281   \n",
      "                              addaV1      0.519947     0.560926     0.778105   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.495000     0.541160     0.841447   \n",
      "                              mcdV1       0.550294     0.423023     0.867588   \n",
      "               no        no   adaptV1     0.677647     0.374755     0.811535   \n",
      "                              addaV1      0.415476     0.491035     0.800784   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.598235     0.451438     0.905921   \n",
      "                              mcdV1       0.548235     0.408464     0.811754   \n",
      "                         temp adaptV1     0.553529     0.398546     0.848728   \n",
      "                              addaV1      0.550166     0.494243     0.778758   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.530735     0.611160     0.849781   \n",
      "                              mcdV1       0.497941     0.525441     0.846754   \n",
      "\n",
      "                                       class_3_acc  test_acc  \n",
      "target_dataset normalize aug  model                           \n",
      "dataset_A      chan      no   adaptV1     0.645934  0.455000  \n",
      "                              addaV1      0.325440  0.355000  \n",
      "                              base             NaN  0.355000  \n",
      "                              dannV1      0.496832  0.500000  \n",
      "                              mcdV1       0.549011  0.390000  \n",
      "                         temp adaptV1     0.696447  0.415000  \n",
      "                              addaV1      0.528114  0.470000  \n",
      "                              base             NaN  0.395000  \n",
      "                              dannV1      0.608883  0.480000  \n",
      "                              mcdV1       0.562729  0.405000  \n",
      "               no        no   adaptV1     0.656685  0.435000  \n",
      "                              addaV1      0.504158  0.400000  \n",
      "                              base             NaN  0.330000  \n",
      "                              dannV1      0.631172  0.445000  \n",
      "                              mcdV1       0.512363  0.420000  \n",
      "                         temp adaptV1     0.664634  0.425000  \n",
      "                              addaV1      0.316722  0.430000  \n",
      "                              base             NaN  0.380000  \n",
      "                              dannV1      0.623352  0.480000  \n",
      "                              mcdV1       0.518773  0.425000  \n",
      "dataset_B      chan      no   adaptV1     0.648089  0.577778  \n",
      "                              addaV1      0.759541  0.650000  \n",
      "                              base             NaN  0.655556  \n",
      "                              dannV1      0.704079  0.608333  \n",
      "                              mcdV1       0.671119  0.597222  \n",
      "                         temp adaptV1     0.803543  0.636111  \n",
      "                              addaV1      0.770933  0.666667  \n",
      "                              base             NaN  0.666667  \n",
      "                              dannV1      0.724988  0.641667  \n",
      "                              mcdV1       0.711259  0.641667  \n",
      "               no        no   adaptV1     0.726737  0.644444  \n",
      "                              addaV1      0.577866  0.569444  \n",
      "                              base             NaN  0.661111  \n",
      "                              dannV1      0.674755  0.658333  \n",
      "                              mcdV1       0.792261  0.638889  \n",
      "                         temp adaptV1     0.828555  0.650000  \n",
      "                              addaV1      0.724355  0.638889  \n",
      "                              base             NaN  0.688889  \n",
      "                              dannV1      0.738928  0.680556  \n",
      "                              mcdV1       0.717156  0.650000  \n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'EA', 'LA', 'aug',\n",
      "       'model', 'backbone', 'source_label_space', 'target_label_space',\n",
      "       'class_0_acc', 'class_1_acc', 'class_2_acc', 'class_3_acc'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       class_0_acc  class_1_acc  class_2_acc  \\\n",
      "target_dataset normalize aug  model                                            \n",
      "dataset_A      chan      no   adaptV1     0.234524     0.356405     0.558889   \n",
      "                              addaV1      0.393810     0.391863     0.508889   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.323571     0.415392     0.582778   \n",
      "                              mcdV1       0.493333     0.426242     0.445556   \n",
      "                         temp adaptV1     0.230000     0.539935     0.527222   \n",
      "                              addaV1      0.406190     0.453170     0.539444   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.418095     0.437614     0.527778   \n",
      "                              mcdV1       0.359286     0.488758     0.489444   \n",
      "               no        no   adaptV1     0.412143     0.606536     0.450000   \n",
      "                              addaV1      0.456667     0.456993     0.334444   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.491429     0.423366     0.380000   \n",
      "                              mcdV1       0.358571     0.551242     0.463889   \n",
      "                         temp adaptV1     0.333571     0.607092     0.612222   \n",
      "                              addaV1      0.434286     0.556993     0.455556   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.480238     0.713301     0.573889   \n",
      "                              mcdV1       0.352143     0.406176     0.502222   \n",
      "dataset_B      chan      no   adaptV1     0.646324     0.372059     0.835088   \n",
      "                              addaV1      0.545736     0.604318     0.798562   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.553529     0.461340     0.872807   \n",
      "                              mcdV1       0.540588     0.630245     0.729254   \n",
      "                         temp adaptV1     0.450735     0.617925     0.860088   \n",
      "                              addaV1      0.499432     0.594103     0.823660   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.565000     0.569412     0.823947   \n",
      "                              mcdV1       0.618529     0.620703     0.783421   \n",
      "               no        no   adaptV1     0.607794     0.467810     0.857281   \n",
      "                              addaV1      0.632724     0.534456     0.673791   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.645588     0.549216     0.881140   \n",
      "                              mcdV1       0.625294     0.588856     0.801447   \n",
      "                         temp adaptV1     0.592794     0.504967     0.890614   \n",
      "                              addaV1      0.585763     0.628802     0.806144   \n",
      "                              base             NaN          NaN          NaN   \n",
      "                              dannV1      0.476471     0.557467     0.861447   \n",
      "                              mcdV1       0.582500     0.651356     0.832281   \n",
      "\n",
      "                                       class_3_acc  test_acc  \n",
      "target_dataset normalize aug  model                           \n",
      "dataset_A      chan      no   adaptV1     0.567985  0.415000  \n",
      "                              addaV1      0.466447  0.400000  \n",
      "                              base             NaN  0.355000  \n",
      "                              dannV1      0.763462  0.500000  \n",
      "                              mcdV1       0.510531  0.445000  \n",
      "                         temp adaptV1     0.657308  0.465000  \n",
      "                              addaV1      0.336868  0.400000  \n",
      "                              base             NaN  0.410000  \n",
      "                              dannV1      0.569780  0.455000  \n",
      "                              mcdV1       0.503883  0.445000  \n",
      "               no        no   adaptV1     0.532747  0.485000  \n",
      "                              addaV1      0.567747  0.455000  \n",
      "                              base             NaN  0.330000  \n",
      "                              dannV1      0.474542  0.435000  \n",
      "                              mcdV1       0.524506  0.445000  \n",
      "                         temp adaptV1     0.576685  0.520000  \n",
      "                              addaV1      0.522106  0.475000  \n",
      "                              base             NaN  0.380000  \n",
      "                              dannV1      0.608498  0.580000  \n",
      "                              mcdV1       0.623883  0.485000  \n",
      "dataset_B      chan      no   adaptV1     0.905524  0.688889  \n",
      "                              addaV1      0.688119  0.661111  \n",
      "                              base             NaN  0.655556  \n",
      "                              dannV1      0.862191  0.680556  \n",
      "                              mcdV1       0.722028  0.644444  \n",
      "                         temp adaptV1     0.956434  0.716667  \n",
      "                              addaV1      0.716169  0.663889  \n",
      "                              base             NaN  0.666667  \n",
      "                              dannV1      0.869231  0.708333  \n",
      "                              mcdV1       0.738835  0.688889  \n",
      "               no        no   adaptV1     0.844918  0.691667  \n",
      "                              addaV1      0.815022  0.658333  \n",
      "                              base             NaN  0.661111  \n",
      "                              dannV1      0.884009  0.738889  \n",
      "                              mcdV1       0.725734  0.680556  \n",
      "                         temp adaptV1     0.926434  0.730556  \n",
      "                              addaV1      0.826217  0.708333  \n",
      "                              base             NaN  0.688889  \n",
      "                              dannV1      0.854918  0.686111  \n",
      "                              mcdV1       0.638835  0.675000  \n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "    'share_adaptV1',\n",
    "    'dannV1',\n",
    "    'mcdV1',\n",
    "    'addaV1'\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_14_3_1\\\\sub\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc','class_0_acc','class_1_acc','class_2_acc','class_3_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_14_3_3\\\\sub\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "table = pd.pivot_table(group_format, values=['test_acc','class_0_acc','class_1_acc','class_2_acc','class_3_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_11_4_1_0\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# # data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "# #                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# modify_col_info(data_result_1)\n",
    "\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# print(\"final avg model compare --\")\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# print(table)\n",
    "\n",
    "# prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_11_4_1_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# # data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "# #                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# modify_col_info(data_result_1)\n",
    "\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# print(\"final avg model compare --\")\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# print(table)\n",
    "\n",
    "# prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_11_4_1_2\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# # data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "# #                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# modify_col_info(data_result_1)\n",
    "\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# print(\"final avg model compare --\")\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b0ad12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_classes_avg_acc', 'class_0_acc',\n",
      "       'class_1_acc', 'class_2_acc', 'class_3_acc', 'shuffle_fold',\n",
      "       'increment_fold', 'test_fold', 'valid_fold', 'target_dataset',\n",
      "       'source_dataset', 'normalize', 'EA', 'LA', 'aug', 'model', 'backbone',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "data cols  Index(['test_acc', 'test_loss', 'test_classes_avg_acc', 'class_0_acc',\n",
      "       'class_1_acc', 'class_2_acc', 'class_3_acc', 'shuffle_fold',\n",
      "       'increment_fold', 'test_fold', 'valid_fold', 'target_dataset',\n",
      "       'source_dataset', 'normalize', 'EA', 'LA', 'aug', 'model', 'backbone',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                                     test_acc                 \\\n",
      "shuffle_fold                                   shuffle_fold_1 shuffle_fold_2   \n",
      "target_dataset normalize aug  model                                            \n",
      "BCI_IV         no        no   MultiDatasetSRDA       0.828993       0.850694   \n",
      "                              adaptV1                0.799913       0.843750   \n",
      "                              addaV1                 0.754340       0.700521   \n",
      "                              base                   0.737847       0.827257   \n",
      "                              dannV1                 0.805122       0.845920   \n",
      "                              mcdV1                  0.847222       0.858941   \n",
      "                         temp MultiDatasetSRDA       0.817274       0.861545   \n",
      "                              adaptV1                0.817708       0.843316   \n",
      "                              addaV1                 0.783420       0.736111   \n",
      "                              base                   0.780382       0.830729   \n",
      "                              dannV1                 0.811632       0.845052   \n",
      "                              mcdV1                  0.868490       0.861979   \n",
      "\n",
      "                                                               \n",
      "shuffle_fold                                   shuffle_fold_3  \n",
      "target_dataset normalize aug  model                            \n",
      "BCI_IV         no        no   MultiDatasetSRDA       0.817708  \n",
      "                              adaptV1                0.781684  \n",
      "                              addaV1                 0.692708  \n",
      "                              base                   0.730903  \n",
      "                              dannV1                 0.782552  \n",
      "                              mcdV1                  0.799045  \n",
      "                         temp MultiDatasetSRDA       0.822917  \n",
      "                              adaptV1                0.761719  \n",
      "                              addaV1                 0.775174  \n",
      "                              base                   0.751302  \n",
      "                              dannV1                 0.784722  \n",
      "                              mcdV1                  0.790365  \n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "    'share_adaptV1',\n",
    "    'dannV1',\n",
    "    'mcdV1',\n",
    "    'addaV1',\n",
    "    'SRDA'\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"BCI_IV\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\private_exp_14_3_1\\\\sub_5\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc','class_0_acc','class_1_acc','class_2_acc','class_3_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# print(table)\n",
    "\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "#                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "print(\"data cols \",data_result_1.columns)\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"shuffle_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=[\"shuffle_fold\"])\n",
    "print(table)\n",
    "\n",
    "# prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\private_exp_14_3_3\\\\sub\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "# # data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list,result_folder = 'predict_folder',file_name = 'ensemble_result.xlsx',\n",
    "# #                          info_file_name = 'model_info.json',info_file_folder='result_folder')\n",
    "# data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# modify_col_info(data_result_1)\n",
    "\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "# print(\"final avg model compare --\")\n",
    "# # table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "# table = pd.pivot_table(group_format, values=['test_acc','class_0_acc','class_1_acc','class_2_acc','class_3_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83779e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

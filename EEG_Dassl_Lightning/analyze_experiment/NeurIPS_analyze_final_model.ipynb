{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b25eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from util.util import generate_data_paths,generate_history_results_path, load_history_data, generate_concat_dataset,filter_history_information,load_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0684de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_history(data_table, pick_cols, col_pick_model=\"val_loss\", pick_min=True, max_epochs=100,min_epoch=10,\n",
    "                      col_pick_max=\"test_acc\", data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        fix_col_pick_model = col_pick_model\n",
    "        history_data = pd.read_csv(path)\n",
    "        available_cols = history_data.columns\n",
    "  \n",
    "        # check if col_pick_model exist\n",
    "        if not col_pick_model in history_data.columns:\n",
    "            print(\"col {} isn't in the history data \".format(col_pick_model))\n",
    "            print(\"use default val_loss as pick col\")\n",
    "            fix_col_pick_model = \"val_loss\"\n",
    "        \n",
    "\n",
    "        # limit total epoch to max epoch\n",
    "        history_epoch = len(history_data)\n",
    "        if history_epoch > max_epochs:\n",
    "            history_data = history_data[:max_epochs]\n",
    "        \n",
    "        if history_epoch > min_epoch:\n",
    "            history_data = history_data[min_epoch:]\n",
    "#             print(\"update history data :\",history_data.head())\n",
    "            history_data = history_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # deal with how to use a metric to pick best model\n",
    "        if pick_min:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmin()\n",
    "        else:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmax()\n",
    "\n",
    "        #\n",
    "\n",
    "        # val_loss_name = 'val_loss' if 'val_loss' in history_data.columns else 'val_loss_x'\n",
    "        metric_pick_model = ['']\n",
    "        # get max possible test_auc score information\n",
    "        best_row_idx = history_data[col_pick_max].argmax()\n",
    "\n",
    "#         print(\"pick row idx : \",pick_row_idx)\n",
    "#         print(\"pick best idx : \",best_row_idx)\n",
    "        \n",
    "        # best_col_pick_model = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # best_col_pick_max = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # \n",
    "        # best_test_auc = history_data.loc[best_row_idx, col_pick_max]\n",
    "        test_class_col = [col for col in pick_cols if \"test_class_\" in col]\n",
    "\n",
    "        history_info_dict = {\n",
    "            \"model_choice\": [\"best_possible_epoch\", \"picked_epoch\"],\n",
    "            \"epoch\": [best_row_idx, pick_row_idx],\n",
    "            col_pick_max: [history_data.loc[best_row_idx, col_pick_max], history_data.loc[pick_row_idx, col_pick_max]],\n",
    "            fix_col_pick_model: [history_data.loc[best_row_idx, fix_col_pick_model],\n",
    "                                 history_data.loc[pick_row_idx, fix_col_pick_model]],\n",
    "            \"history_path\": [path, path]\n",
    "        }\n",
    "\n",
    "        history_information = pd.DataFrame(history_info_dict)\n",
    "        history_information_table.append(history_information)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def generate_history_results_path(row, full_result_path):\n",
    "    remain='default\\\\version_0\\\\metrics.csv'\n",
    "    test_fold = row['test_fold']\n",
    "    shuffle_fold = row['shuffle_fold']\n",
    "    increment_fold = row['increment_fold']\n",
    "    valid_fold = row['valid_fold']\n",
    "    history_path = os.path.join(full_result_path,test_fold, shuffle_fold,increment_fold, valid_fold,\n",
    "                                remain)\n",
    "#     print(\"current history path : \",history_path)\n",
    "    return history_path\n",
    "def load_history_data(data_table, pick_cols, data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    print(\"temp col : \",temp.columns)\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        history_data = pd.read_csv(path)\n",
    "        history_data[data_path_col] = [path] * len(history_data)\n",
    "        history_information_table.append(history_data)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def load_data(data_paths, result_folder, result_file_name, info_file_name, load_history=False):\n",
    "    list_data = []\n",
    "    if len(data_paths) ==0:\n",
    "        print(\"no data path \")\n",
    "    for data_path in data_paths:\n",
    "        result_folder_path = os.path.join(data_path, result_folder)\n",
    "        result_data_path = os.path.join(result_folder_path, result_file_name)\n",
    "        # check if file result exists\n",
    "        if os.path.exists(result_data_path):\n",
    "            data = pd.read_excel(result_data_path)\n",
    "            data_size = len(data)\n",
    "            info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "            if os.path.exists(info_data_path):\n",
    "                with open(info_data_path) as f:\n",
    "                    info_data = json.load(f)\n",
    "                    extra_fields = info_data[\"EXTRA_FIELDS\"]\n",
    "                    field_names = list(extra_fields.keys())\n",
    "                    for field_name in field_names:\n",
    "                        if extra_fields[field_name] == []:\n",
    "                            extra_fields[field_name] = None\n",
    "                        data[field_name] = data_size*[extra_fields[field_name]]\n",
    "                list_data.append(data)\n",
    "            else:\n",
    "                print(\"no data info for {} \".format(result_data_path))\n",
    "\n",
    "            if load_history:\n",
    "                data['history_path'] = data.apply(lambda row: generate_history_results_path(row, data_path), axis=1)\n",
    "#                 print(\"load current history path : \",data['history_path'].values[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"the current data path {} does not exist \".format(result_data_path))\n",
    "\n",
    "    final_data = pd.concat(list_data).reset_index(drop=True)\n",
    "    return final_data\n",
    "# prefix_lists=[augmentation_prefix,norm_prefix,model_prefix,dataset_prefix]\n",
    "def load_experiment_data(common_path, prefix_lists=None,pick_cols=None,\n",
    "                         col_pick_model=None,col_pick_model_min=True,\n",
    "                         new_col_generate=None,load_history = False):\n",
    "\n",
    "    result_folder = 'result_folder'\n",
    "    file_name = 'model_result.xlsx'\n",
    "    info_file_name = 'model_info.json'\n",
    "\n",
    "    list_full_path = generate_data_paths(common_path, prefix_lists, [])\n",
    "    data_result = load_data(list_full_path, result_folder, file_name, info_file_name, load_history=load_history)\n",
    "    data_cols = data_result.columns\n",
    "\n",
    "    pick_cols = ['test_fold', 'shuffle_fold', 'increment_fold',\n",
    "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
    "       'model', 'source_label_space', 'target_label_space','history_path']\n",
    "    if pick_cols is None:\n",
    "        pick_cols = list(data_cols)\n",
    "\n",
    "#     if new_col_generate is not None:\n",
    "#         for col_generate in new_col_generate:    \n",
    "#             new_col_name = col_generate[0]\n",
    "#             func = col_generate[1]\n",
    "#             data_result[new_col_name] = data_result.apply(lambda row: func(row,data_cols), axis=1)\n",
    "#             pick_cols.append(new_col_name)\n",
    "    if col_pick_model is None:\n",
    "        col_pick_model = 'val_loss'\n",
    "    pick_min = col_pick_model_min\n",
    "    print(\"data result cols : \",data_result.columns)\n",
    "    if load_history:\n",
    "        summary = summarize_history(data_result, pick_cols)\n",
    "        history_data = load_history_data(data_result, pick_cols)\n",
    "        return data_result,history_data,summary\n",
    "    return data_result\n",
    "\n",
    "\n",
    "\n",
    "def modify_col_info(data_result):\n",
    "    data_result['increment_fold'] = data_result['increment_fold'].replace(\n",
    "    ['increment_fold_1', 'increment_fold_2', 'increment_fold_3'], ['1', '2', '3'])\n",
    "    data_result['valid_fold'] = data_result['valid_fold'].replace(\n",
    "        ['valid_fold_1', 'valid_fold_2', 'valid_fold_3','valid_fold_4'], ['1', '2', '3','4'])\n",
    "    data_result['test_fold'] = data_result['test_fold'].replace(\n",
    "        ['test_fold_1','test_fold_2','test_fold_3','test_fold_4','test_fold_5'], ['1','2','3','4','5'])\n",
    "    data_result['aug'] = data_result['aug'].replace(\n",
    "        ['no_aug', 'temporal_aug'], ['no', 'temp'])\n",
    "    data_result['normalize'] = data_result['normalize'].replace(\n",
    "        ['chan_norm', 'no_norm'], ['chan', 'no'])\n",
    "    data_result['model'] = data_result['model'].replace(\n",
    "        ['ComponentAdaptation', 'BaseModel', 'MultiDatasetAdaptation','MultiDatasetAdaptationV1'], ['component', 'base','adapt','adaptV1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68da812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "    'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e3be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d95a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       test_acc                                \\\n",
      "test_fold                                     1         2         3         4   \n",
      "target_dataset normalize aug  model                                             \n",
      "dataset_A      chan      no   adapt    0.506250  0.431250  0.456250  0.468750   \n",
      "                              adaptV1  0.587500  0.568750  0.550000  0.487500   \n",
      "                              base     0.275000  0.343750  0.356250  0.268750   \n",
      "                         temp adapt    0.375000  0.512500  0.331250  0.443750   \n",
      "                              adaptV1  0.593750  0.587500  0.531250  0.550000   \n",
      "                              base     0.281250  0.381250  0.337500  0.293750   \n",
      "               no        no   adapt    0.475000  0.475000  0.343750  0.506250   \n",
      "                              adaptV1  0.225000  0.375000  0.356250  0.250000   \n",
      "                              base     0.231250  0.300000  0.306250  0.225000   \n",
      "                         temp adapt    0.481250  0.487500  0.431250  0.493750   \n",
      "                              adaptV1  0.300000  0.468750  0.356250  0.250000   \n",
      "                              base     0.337500  0.418750  0.381250  0.568750   \n",
      "dataset_B      chan      no   adapt    0.496528  0.520833  0.496528  0.538194   \n",
      "                              adaptV1  0.534722  0.541667  0.604167  0.618056   \n",
      "                              base     0.369792  0.343750  0.395833  0.255208   \n",
      "                         temp adapt    0.541667  0.534722  0.565972  0.590278   \n",
      "                              adaptV1  0.538194  0.607639  0.593750  0.614583   \n",
      "                              base     0.458333  0.447917  0.572917  0.651042   \n",
      "               no        no   adapt    0.489583  0.520833  0.600694  0.600694   \n",
      "                              adaptV1  0.520833  0.517361  0.590278  0.472222   \n",
      "                              base     0.401042  0.317708  0.369792  0.385417   \n",
      "                         temp adapt    0.479167  0.513889  0.625000  0.586806   \n",
      "                              adaptV1  0.520833  0.486111  0.510417  0.399306   \n",
      "                              base     0.526042  0.437500  0.442708  0.541667   \n",
      "\n",
      "                                                 \n",
      "test_fold                                     5  \n",
      "target_dataset normalize aug  model              \n",
      "dataset_A      chan      no   adapt    0.418750  \n",
      "                              adaptV1  0.468750  \n",
      "                              base     0.312500  \n",
      "                         temp adapt    0.462500  \n",
      "                              adaptV1  0.418750  \n",
      "                              base     0.250000  \n",
      "               no        no   adapt    0.437500  \n",
      "                              adaptV1  0.325000  \n",
      "                              base     0.200000  \n",
      "                         temp adapt    0.475000  \n",
      "                              adaptV1  0.337500  \n",
      "                              base     0.318750  \n",
      "dataset_B      chan      no   adapt    0.572917  \n",
      "                              adaptV1  0.583333  \n",
      "                              base     0.406250  \n",
      "                         temp adapt    0.572917  \n",
      "                              adaptV1  0.611111  \n",
      "                              base     0.541667  \n",
      "               no        no   adapt    0.559028  \n",
      "                              adaptV1  0.590278  \n",
      "                              base     0.458333  \n",
      "                         temp adapt    0.586806  \n",
      "                              adaptV1  0.555556  \n",
      "                              base     0.588542  \n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adapt    0.456250\n",
      "                              adaptV1  0.532500\n",
      "                              base     0.311250\n",
      "                         temp adapt    0.425000\n",
      "                              adaptV1  0.536250\n",
      "                              base     0.308750\n",
      "               no        no   adapt    0.447500\n",
      "                              adaptV1  0.306250\n",
      "                              base     0.252500\n",
      "                         temp adapt    0.473750\n",
      "                              adaptV1  0.342500\n",
      "                              base     0.405000\n",
      "dataset_B      chan      no   adapt    0.525000\n",
      "                              adaptV1  0.576389\n",
      "                              base     0.354167\n",
      "                         temp adapt    0.561111\n",
      "                              adaptV1  0.593056\n",
      "                              base     0.534375\n",
      "               no        no   adapt    0.554167\n",
      "                              adaptV1  0.538194\n",
      "                              base     0.386458\n",
      "                         temp adapt    0.558333\n",
      "                              adaptV1  0.494444\n",
      "                              base     0.507292\n"
     ]
    }
   ],
   "source": [
    "save_data_folder = \"NeurIPS\\data\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['test_fold'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)\n",
    "# output_path = os.path.join(save_data_folder,'experiment_3_avg.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de99457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                     test_acc\n",
      "target_dataset normalize aug  model          \n",
      "dataset_A      chan      no   adapt  0.490000\n",
      "                              base   0.335000\n",
      "                         temp adapt  0.415000\n",
      "                              base   0.410000\n",
      "               no        no   adapt  0.500000\n",
      "                              base   0.335000\n",
      "                         temp adapt  0.465000\n",
      "                              base   0.425000\n",
      "dataset_B      chan      no   adapt  0.511111\n",
      "                              base   0.508333\n",
      "                         temp adapt  0.597222\n",
      "                              base   0.529167\n",
      "               no        no   adapt  0.550000\n",
      "                              base   0.491667\n",
      "                         temp adapt  0.552778\n",
      "                              base   0.566667\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_3\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6b29fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current data path C:\\wduong_folder\\Dassl.pytorch-master\\NeurIPS_competition\\EEG_Dassl_Lightning\\NeurIPS_competition\\final_result_4\\no_aug\\chan_norm\\adaptation\\dataset_A\\model\\result_folder\\model_result.xlsx does not exist \n",
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'target_dataset', 'source_dataset', 'normalize', 'aug', 'model',\n",
      "       'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "final avg model compare --\n",
      "                                       test_acc\n",
      "target_dataset normalize aug  model            \n",
      "dataset_A      chan      no   adaptV1  0.500000\n",
      "                              base     0.400000\n",
      "                         temp adaptV1  0.580000\n",
      "                              base     0.465000\n",
      "               no        no   adaptV1  0.295000\n",
      "                              base     0.305000\n",
      "                         temp adaptV1  0.280000\n",
      "                              base     0.530000\n",
      "dataset_B      chan      no   adaptV1  0.619444\n",
      "                              base     0.458333\n",
      "                         temp adaptV1  0.608333\n",
      "                              base     0.570833\n",
      "               no        no   adaptV1  0.500000\n",
      "                              base     0.475000\n",
      "                         temp adaptV1  0.494444\n",
      "                              base     0.570833\n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'adaptationV1',\n",
    "\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"dataset_A\",\n",
    "    \"dataset_B\",\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug',\n",
    "#     'T_F_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\NeurIPS_competition\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\final_result_4\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "modify_col_info(data_result_1)\n",
    "\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"test_fold\",\"model\"],as_index=False).mean()\n",
    "print(\"final avg model compare --\")\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c89f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

use determinstic 
cfg file setup :  DATAMANAGER:
  DATALOADER:
    LIST_TRAIN_U:
      BATCH_SIZES: [64, 64, 64]
      SAMPLERS: ['RandomSampler', 'RandomSampler', 'RandomSampler']
    NUM_WORKERS: 0
    TEST:
      BATCH_SIZE: 64
      SAMPLER: SequentialSampler
    TRAIN_U:
      BATCH_SIZE: 16
      N_DOMAIN: 0
      SAME_AS_X: True
      SAMPLER: RandomSampler
    TRAIN_X:
      BATCH_SIZE: 16
      N_DOMAIN: 0
      SAMPLER: RandomSampler
    VALID:
      BATCH_SIZE: 64
      N_DOMAIN: 0
      SAMPLER: SequentialSampler
  DATASET:
    AUGMENTATION:
      NAME: 
      PARAMS:
        DATASET_NAME: BCI_IV
        MAX_FIX_TRIAL: -1
        MAX_TRIAL_MUL: 3
        N_SEGMENT: 4
    DIR: final_MI_A_1
    EA: False
    FILENAME: dataset_A_0/NeurIPS_TL.mat
    FILTERBANK:
      USE_FILTERBANK: False
      freq_interval: 4
    NAME: MultiDataset
    ROOT: C:/wduong_folder/Dassl.pytorch-master/NeurIPS_competition/EEG_Lightning//da_dataset/NeurIPS_2
    SETUP:
      INCREMENT_FOLD:
        CURRENT_INCREMENT_FOLD: 1
        END_INCREMENT_FOLD: 1
        INCREMENT_FOLD_PREFIX: increment_fold
        INCREMENT_TRAIN_SUGJECT: 0
        N_INCREMENT_FOLDS: 1
        START_INCREMENT_FOLD: 1
        START_NUM_TRAIN_SUGJECT: 1
      SHUFFLE_TRAIN_VALID_FOLD:
        CURRENT_SHUFFLE_FOLD: 1
        END_SHUFFLE_FOLD: 1
        N_SHUFFLE_FOLDS: 1
        SET_FIX_SEED: False
        SHUFFLE_FOLD_PREFIX: shuffle_fold
        SHUFFLE_SEED: []
        START_SHUFFLE_FOLD: 1
      SOURCE_DATASET_NAMES: []
      TARGET_DATASET_NAME: dataset_A_0
      TARGET_DATASET_NAMES: []
      TEST_FOLD:
        CURRENT_TEST_FOLD: 1
        END_TEST_FOLD: 1
        NUM_TEST_SUBJECTS: -1
        N_TEST_FOLDS: 1
        SAME_AS_VALID: True
        START_TEST_FOLD: 1
        TEST_FOLD_PREFIX: test_fold
        TEST_SUBJECT_INDEX: []
        TRAIN_VALID_DATA_RATIO: -1
        WITHIN_SUBJECTS: True
      VALID_FOLD:
        CROSS_SUBJECTS: False
        CURRENT_VALID_FOLD: 1
        DOMAIN_CLASS_WEIGHT: True
        END_VALID_FOLD: 5
        N_VALID_FOLDS: 5
        SOURCE_DOMAIN_CLASS_WEIGHT: True
        START_VALID_FOLD: 1
        TOTAL_CLASS_WEIGHT: True
        TRAIN_DATA_RATIO: -1
        VALID_FOLD_PREFIX: valid_fold
        WITHIN_SUBJECTS: True
    TEST_DATA_FILE: dataset_A_0/NeurIPS_TL.mat
    TEST_DIR: final_MI_test_A_1
    TEST_SET_FILENAME: 
    USE_Euclidean_Aligment: True
    extra_files: ['dataset_A_0/LA/dataset_1.mat', 'dataset_A_0/LA/dataset_2.mat', 'dataset_A_0/LA/dataset_3.mat']
    r_op_file: dataset_A_0/dataset_A_0_r_op.mat
    source_dataset_LA: False
    target_dataset_relabelled: False
  MANAGER_TYPE: multi_datasetV2
  RESULT_FOLDER: result_folder
DISPLAY_INFO:
  DATASET: True
  DataManager: True
  TRAINER: True
  writer: False
EXTRA_FIELDS:
  EA: True
  LA: True
  aug: no_aug
  backbone: eegnet
  model: MultiDatasetMCDV1
  normalize: no_norm
  source_dataset: ['cho2017', 'physionet', 'BCI_IV']
  source_label_space: [2, 4, 4]
  target_dataset: dataset_A_0
  target_label_space: 4
INPUT:
  NO_TRANSFORM: True
  SIZE: (17, 384)
  TRANSFORMS: []
LIGHTNING_MODEL:
  ACTIVE_LEARNING:
    USE_MODEL_UPDATE_DIR: 
    ensemble_confidence_level: -1
  COMPONENTS:
    BACKBONE:
      FREEZE: False
      NAME: eegnet
      PARAMS:
        D: 2
        F1: 8
        F2: 16
        avg_pool_1: 4
        avg_pool_2: 8
        drop_prob: 0.25
        kern_legnth: 64
        num_ch: 17
        samples: 384
        sep_kern_length: 16
      PRETRAINED: False
      PRETRAINED_PATH: 
    LAST_FC:
      NAME: max_norm
      max_norm: 0.5
    LAYER:
      NAME: EEGNetConv3
      PARAMS:
        F2: 16
        avg_pool_1: 4
        avg_pool_2: 8
        drop_prob: 0.25
        samples: 256
        sep_kern_length: 16
  PRETRAIN:
    DIR: 
    USE_BEST: False
  TRAINER:
    CDAN:
      lmda: 1.0
      use_entropy: False
      use_projection: False
    DAN:
      GaussianKernel:
        alpha: [0.5, 1.0, 2.0]
        sigma: []
        track_running_stats: True
      linear: False
      lmda: 1.0
      trade_off: 1.0
    DANN:
      lmda: 1.0
    EXTRA:
      PRETRAIN_SOURCE_LOSS_RATIO: 1.0
      PRETRAIN_TARGET_LOSS_RATIO: 0.2
      SOURCE_LOSS_RATIO: 0.2
      SOURCE_PRE_TRAIN_EPOCHS: 8
      TARGET_LOSS_RATIO: 1.0
    EpiDG:
      loss_weight_epic: 0.8
      loss_weight_epif: 0.8
      loss_weight_epir: 0.8
      start_train_classifier: 35
      start_train_feature: 35
      warm_up_DS: 25
      warn_up_AGG: 35
    MLDG:
      alpha: 0.1
      inner_lr: 0.1
      num_inner_loop: 1
      num_test_subject: 1
      percent_test_subject: 0.0
    NAME: MultiDatasetMCDV1
    PARAMS:
      pretrain: False
      pretrain_epochs: 0
LIGHTNING_TRAINER:
  CHECKPOINT:
    every_n_val_epochs: 1
    filename: best
    monitor: val_loss
    save_last: True
    save_top_k: 1
  LOGGER:
    
  early_stop:
    params:
      
    use_early_stop: False
  multiple_trainloader_mode: max_size_cycle
  num_sanity_val_steps: 0
  profiler: simple
  progress_bar_refresh_rate: 100
  stochastic_weight_avg: False
OPTIM:
  BASE_LR_MULT: 0.1
  COSINDECAY:
    LAST_EPOCH: -1
    MAX_LR: 0.01
    WARM_DROP: 1.0
    WARM_UP: 10
  GAMMA: 0.1
  LR_SCHEDULER: single_step
  MAX_EPOCH: 15
  NEW_LAYERS: ()
  OPTIMIZER:
    NAME: adam
    PARAMS:
      lr: 0.001
      weight_decay: 0.0
  SCHEDULER:
    NAME: exponential
    PARAMS:
      gamma: 1.0
  STAGED_LR: False
  STEPSIZE: (10,)
OUTPUT_DIR: C:/wduong_folder/Dassl.pytorch-master/NeurIPS_competition/EEG_Lightning//NeurIPS_2/task_2_final_2/LA_EA//no_aug/no_norm/mcdV1/dataset_A_0/model
RESUME: 
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  EVAL_FREQ: 1
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: bigger_one
  PRINT_FREQ: 10
  SAVE_HISTORY_RECORD: True
  SAVE_LAST_EPOCH: True
USE_CUDA: True
VERBOSE: True
VERSION: 1
history_dir: 
output_dir: 
use data manager for domain adaptation
Loading dataset: MultiDataset
data root :  C:\wduong_folder\Dassl.pytorch-master\NeurIPS_competition\EEG_Lightning\da_dataset\NeurIPS_2
dataset dir :  final_MI_A_1
file name :  dataset_A_0/NeurIPS_TL.mat
r_op file path :  C:\wduong_folder\Dassl.pytorch-master\NeurIPS_competition\EEG_Lightning\da_dataset\NeurIPS_2\final_MI_A_1\dataset_A_0/dataset_A_0_r_op.mat

there is test file 
run custom EA
only use r-op for subjects [0]
only use r-op for subjects [0]
only use r-op for subjects [0]
use exist r_op
use exist r_op
use exist r_op
train subjects :  [0]
Train subject 0 has shape : (80, 1, 17, 384), with range scale (0.598308265209198,-1.0431071519851685) 
test subjects :  [0]
test subject 0 has shape : (20, 1, 17, 384), with range scale (0.4419059157371521,-0.5427009463310242)  
valid subjects :  [0]
valid subject 0 has shape : (20, 1, 17, 384), with range scale (0.4419059157371521,-0.5427009463310242)  
the labels ratio of whole dataset : [3.8095238095238093, 4.444444444444445, 3.8095238095238093, 4.0]
source dataset idx :  0
run custom EA
generate new r_op
source_data subject_idx 0 has shape : (200, 17, 384), with range scale (2.9441733360290527,-2.953942060470581) 
source_data subject_idx 1 has shape : (200, 17, 384), with range scale (2.274538516998291,-1.6750545501708984) 
source dataset idx :  1
run custom EA
generate new r_op
source_data subject_idx 0 has shape : (92, 17, 384), with range scale (0.6530978083610535,-0.6130669116973877) 
source_data subject_idx 1 has shape : (89, 17, 384), with range scale (0.60467529296875,-0.8001972436904907) 
source dataset idx :  2
run custom EA
generate new r_op
source_data subject_idx 0 has shape : (576, 17, 384), with range scale (0.7512149810791016,-1.0368345975875854) 
source_data subject_idx 1 has shape : (576, 17, 384), with range scale (0.782868504524231,-0.7371304631233215) 
only use r-op for subjects [0]
use exist r_op
unlabel data subject_idx 0 has shape : (200, 17, 384), with range scale (0.8025001883506775,-1.0645889043807983) 
Loading trainer: MultiDatasetMCDV1
Params :  FREEZE: False
NAME: eegnet
PARAMS:
  D: 2
  F1: 8
  F2: 16
  avg_pool_1: 4
  avg_pool_2: 8
  drop_prob: 0.25
  kern_legnth: 64
  num_ch: 17
  samples: 384
  sep_kern_length: 16
PRETRAINED: False
PRETRAINED_PATH: 
Building F
Building CommonFeature
Backbone: eegnet
params set up :  {'kern_legnth': 64, 'num_ch': 17, 'samples': 384, 'F1': 8, 'D': 2, 'F2': 16, 'drop_prob': 0.25, 'avg_pool_1': 4, 'avg_pool_2': 8, 'sep_kern_length': 16}
pretrain :  False
pretrain path : 
Building Target Classifier
use max norm constraint on last FC
use max norm constraint on last FC
Building SourceClassifiers
source domains label size :  [2, 4, 4]
use max norm constraint on last FC
use max norm constraint on last FC
use max norm constraint on last FC
train x size :  80
size of train loader :  5
total source domain :  3
available batch for source domain :  3
loader dict :  {'target_loader': <torch.utils.data.dataloader.DataLoader object at 0x000002CE9C011148>, 'source_loader': [<torch.utils.data.dataloader.DataLoader object at 0x000002CE9E1CCB48>, <torch.utils.data.dataloader.DataLoader object at 0x000002CE9E1CC9C8>, <torch.utils.data.dataloader.DataLoader object at 0x000002CE9E1CC848>], 'unlabel_loader': <torch.utils.data.dataloader.DataLoader object at 0x000002CE9E1CC6C8>}
size of val loader :  1
size of test loader :  1
Training: -1it [00:00, ?it/s]target class weight :  tensor([3.8095, 4.4444, 3.8095, 4.0000], device='cuda:0')
source domain weight :  [tensor([2., 2.], device='cuda:0'), tensor([3.9348, 4.1136, 4.0222, 3.9348], device='cuda:0'), tensor([4., 4., 4., 4.], device='cuda:0')]
Training:   0%|          | 0/20 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/20 [00:00<?, ?it/s] Epoch 0: 100%|##########| 20/20 [00:03<00:00,  6.26it/s]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2 [00:00<?, ?it/s][AEpoch 0: 100%|##########| 20/20 [00:03<00:00,  6.20it/s, v_num=0_1, val_loss=27.90, val_acc=0.250, val_acc_1=0.150, val_acc_2=0.300]
                                                 [AEpoch 0:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=27.90, val_acc=0.250, val_acc_1=0.150, val_acc_2=0.300]         Epoch 1:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=27.90, val_acc=0.250, val_acc_1=0.150, val_acc_2=0.300]Epoch 1: 100%|##########| 20/20 [00:02<00:00,  9.07it/s, v_num=0_1, val_loss=27.90, val_acc=0.250, val_acc_1=0.150, val_acc_2=0.300]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2 [00:00<?, ?it/s][AEpoch 1: 100%|##########| 20/20 [00:02<00:00,  8.64it/s, v_num=0_1, val_loss=27.70, val_acc=0.350, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.309, Train_loss_A=1.700, Train_source_loss=1.150, Train_target_loss=2.780, Train_loss_B=2.710, Train_loss_C=0.0317]
                                                 [AEpoch 1:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=27.70, val_acc=0.350, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.309, Train_loss_A=1.700, Train_source_loss=1.150, Train_target_loss=2.780, Train_loss_B=2.710, Train_loss_C=0.0317]         Epoch 2:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=27.70, val_acc=0.350, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.309, Train_loss_A=1.700, Train_source_loss=1.150, Train_target_loss=2.780, Train_loss_B=2.710, Train_loss_C=0.0317]Epoch 2: 100%|##########| 20/20 [00:02<00:00,  8.96it/s, v_num=0_1, val_loss=27.70, val_acc=0.350, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.309, Train_loss_A=1.700, Train_source_loss=1.150, Train_target_loss=2.780, Train_loss_B=2.710, Train_loss_C=0.0317]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2 [00:00<?, ?it/s][AEpoch 2: 100%|##########| 20/20 [00:02<00:00,  8.57it/s, v_num=0_1, val_loss=27.50, val_acc=0.300, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.503, Train_loss_A=1.660, Train_source_loss=1.130, Train_target_loss=2.680, Train_loss_B=2.670, Train_loss_C=0.0208]
                                                 [AEpoch 2:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=27.50, val_acc=0.300, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.503, Train_loss_A=1.660, Train_source_loss=1.130, Train_target_loss=2.680, Train_loss_B=2.670, Train_loss_C=0.0208]         Epoch 3:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=27.50, val_acc=0.300, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.503, Train_loss_A=1.660, Train_source_loss=1.130, Train_target_loss=2.680, Train_loss_B=2.670, Train_loss_C=0.0208]Epoch 3: 100%|##########| 20/20 [00:02<00:00,  8.97it/s, v_num=0_1, val_loss=27.50, val_acc=0.300, val_acc_1=0.200, val_acc_2=0.350, Train_acc=0.503, Train_loss_A=1.660, Train_source_loss=1.130, Train_target_loss=2.680, Train_loss_B=2.670, Train_loss_C=0.0208]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/2 [00:00<?, ?it/s][AEpoch 3: 100%|##########| 20/20 [00:02<00:00,  8.58it/s, v_num=0_1, val_loss=26.20, val_acc=0.350, val_acc_1=0.300, val_acc_2=0.350, Train_acc=0.549, Train_loss_A=1.630, Train_source_loss=1.100, Train_target_loss=2.640, Train_loss_B=2.610, Train_loss_C=0.0178]
                                                 [AEpoch 3:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=26.20, val_acc=0.350, val_acc_1=0.300, val_acc_2=0.350, Train_acc=0.549, Train_loss_A=1.630, Train_source_loss=1.100, Train_target_loss=2.640, Train_loss_B=2.610, Train_loss_C=0.0178]         Epoch 4:   0%|          | 0/20 [00:00<?, ?it/s, v_num=0_1, val_loss=26.20, val_acc=0.350, val_acc_1=0.300, val_acc_2=0.350, Train_acc=0.549, Train_loss_A=1.630, Train_source_loss=1.100, Train_target_loss=2.640, Train_loss_B=2.610, Train_loss_C=0.0178]
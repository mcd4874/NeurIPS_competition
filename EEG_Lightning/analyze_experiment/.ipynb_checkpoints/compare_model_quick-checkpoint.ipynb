{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from util.util import generate_data_paths,generate_history_results_path, load_history_data, generate_concat_dataset,filter_history_information,load_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def generate_history_results_path(row, full_result_path):\n",
    "#     history_folder = 'history'\n",
    "#     history_file = 'history.csv'\n",
    "#     test_fold = row['test_fold']\n",
    "#     increment_fold = row['increment_fold']\n",
    "#     valid_fold = row['valid_fold']\n",
    "#     provided_valid_fold = valid_fold.split(\"_\")[-1]\n",
    "#     history_path = os.path.join(full_result_path, history_folder, test_fold, increment_fold, str(provided_valid_fold),\n",
    "#                                 history_file)\n",
    "#     return history_path\n",
    "def load_data(data_paths, result_folder, result_file_name, info_file_name, load_history=False):\n",
    "    list_data = []\n",
    "    if len(data_paths) ==0:\n",
    "        print(\"no data path \")\n",
    "    for data_path in data_paths:\n",
    "        result_folder_path = os.path.join(data_path, result_folder)\n",
    "        result_data_path = os.path.join(result_folder_path, result_file_name)\n",
    "        # check if file result exists\n",
    "        if os.path.exists(result_data_path):\n",
    "            data = pd.read_excel(result_data_path)\n",
    "\n",
    "            info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "            if os.path.exists(info_data_path):\n",
    "                with open(info_data_path) as f:\n",
    "                    info_data = json.load(f)\n",
    "                    extra_fields = info_data[\"EXTRA_FIELDS\"]\n",
    "                    field_names = list(extra_fields.keys())\n",
    "                    for field_name in field_names:\n",
    "                        data[field_name] = extra_fields[field_name]\n",
    "                list_data.append(data)\n",
    "            else:\n",
    "                print(\"no data info for {} \".format(result_data_path))\n",
    "\n",
    "            if load_history:\n",
    "                data['history_path'] = data.apply(lambda row: generate_history_results_path(row, data_path), axis=1)\n",
    "        #                 print(data['history_path'])\n",
    "\n",
    "        else:\n",
    "            print(\"the current data path {} does not exist \".format(result_data_path))\n",
    "\n",
    "    final_data = pd.concat(list_data).reset_index(drop=True)\n",
    "    return final_data\n",
    "\n",
    "# def summarize_history(data_table, pick_cols, col_pick_model=\"val_loss\", pick_min=True, max_epochs=100,\n",
    "#                       col_pick_max=\"test_accuracy\", data_path_col='history_path'):\n",
    "#     if data_path_col not in data_table.columns:\n",
    "#         print(\"there are no history path to load history data\")\n",
    "#         return\n",
    "#     history_information_table = []\n",
    "#     temp = data_table[pick_cols]\n",
    "#     history_cols = temp[data_path_col]\n",
    "#     for path in history_cols.values:\n",
    "#         fix_col_pick_model = col_pick_model\n",
    "#         history_data = pd.read_csv(path)\n",
    "#         available_cols = history_data.columns\n",
    "#         # modify val_loss for target model across experiment\n",
    "#         if 'val_loss_x' in available_cols:\n",
    "#             history_data = history_data.rename(columns={'val_loss_x': 'val_loss'}, inplace=False)\n",
    "\n",
    "#         # check if col_pick_model exist\n",
    "#         if not col_pick_model in history_data.columns:\n",
    "#             print(\"col {} isn't in the history data \".format(col_pick_model))\n",
    "#             print(\"use default val_loss as pick col\")\n",
    "#             fix_col_pick_model = \"val_loss\"\n",
    "\n",
    "#         # limit total epoch to max epoch\n",
    "#         max_history_epoch = len(history_data)\n",
    "#         if max_history_epoch > max_epochs:\n",
    "#             history_data = history_data[:max_epochs]\n",
    "\n",
    "#         # deal with how to use a metric to pick best model\n",
    "#         if pick_min:\n",
    "#             pick_row_idx = history_data[fix_col_pick_model].argmin()\n",
    "#         else:\n",
    "#             pick_row_idx = history_data[fix_col_pick_model].argmax()\n",
    "\n",
    "#         #\n",
    "\n",
    "#         # val_loss_name = 'val_loss' if 'val_loss' in history_data.columns else 'val_loss_x'\n",
    "#         metric_pick_model = ['']\n",
    "#         # get max possible test_auc score information\n",
    "#         best_row_idx = history_data[col_pick_max].argmax()\n",
    "\n",
    "#         # best_col_pick_model = history_data.loc[best_row_idx, col_pick_max]\n",
    "#         # best_col_pick_max = history_data.loc[best_row_idx, col_pick_max]\n",
    "#         # \n",
    "#         # best_test_auc = history_data.loc[best_row_idx, col_pick_max]\n",
    "#         test_class_col = [col for col in pick_cols if \"test_class_\" in col]\n",
    "\n",
    "#         history_info_dict = {\n",
    "#             \"model_choice\": [\"best_possible_epoch\", \"picked_epoch\"],\n",
    "#             \"epoch\": [best_row_idx, pick_row_idx],\n",
    "#             col_pick_max: [history_data.loc[best_row_idx, col_pick_max], history_data.loc[pick_row_idx, col_pick_max]],\n",
    "#             fix_col_pick_model: [history_data.loc[best_row_idx, fix_col_pick_model],\n",
    "#                                  history_data.loc[pick_row_idx, fix_col_pick_model]],\n",
    "#             \"history_path\": [path, path]\n",
    "#         }\n",
    "#         for test_class in test_class_col:\n",
    "#             best_test_class_acc = history_data.loc[best_row_idx, test_class]\n",
    "#             pick_test_class_acc = history_data.loc[pick_row_idx, test_class]\n",
    "#             history_info_dict[test_class] = [best_test_class_acc, pick_test_class_acc]\n",
    "\n",
    "#         history_information = pd.DataFrame(history_info_dict)\n",
    "#         history_information_table.append(history_information)\n",
    "#     history_information_table = pd.concat(history_information_table)\n",
    "#     merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "#     return merge_table\n",
    "def load_experiment_data(common_path, model_list, seed_list=None, norm_list=None, model_data_prefix=None,pick_cols=None,\n",
    "                         col_pick_model=None,col_pick_model_min=True,\n",
    "                         new_col_generate=None,load_history = False):\n",
    "\n",
    "    if seed_list is None:\n",
    "        seed_list = [\n",
    "            \"seed_v0\",\n",
    "            \"seed_v1\",\n",
    "            \"seed_v2\"\n",
    "        ]\n",
    "    if norm_list is None:\n",
    "        norm_list = [\n",
    "            'norm_none',\n",
    "        ]\n",
    "\n",
    "    if model_data_prefix is None:\n",
    "        model_data_prefix = [\n",
    "                \"BCI_IV\",\n",
    "        ]\n",
    "    result_folder = 'results_v1'\n",
    "    file_name = 'model_result..xlsx'\n",
    "    info_file_name = 'model_info.json'\n",
    "    prefix_lists = [model_list, seed_list , norm_list, model_data_prefix]\n",
    "    print(\"common path : \",common_path)\n",
    "    print(\"prefix lists : \",prefix_lists)\n",
    "    list_full_path = generate_data_paths(common_path, prefix_lists, [])\n",
    "    print(\"some list full path : \",list_full_path)\n",
    "    data_result = load_data(list_full_path, result_folder, file_name, info_file_name, load_history=load_history)\n",
    "    data_cols = data_result.columns\n",
    "\n",
    "    #get test_class_{}_acc col\n",
    "    test_class_col = [col for col in data_cols if \"test_class_\" in col]\n",
    "#     print(\"test clas col : \",test_class_col)\n",
    "    if pick_cols is None:\n",
    "        pick_cols = [\"seed\", \"normalize\", \"dataset\", \"test_fold\", \"increment_fold\", \"valid_fold\", \"model\"]\n",
    "        if load_history:\n",
    "            pick_cols = pick_cols + [\"history\"]\n",
    "    pick_cols = pick_cols+test_class_col\n",
    "\n",
    "    if new_col_generate is not None:\n",
    "        for col_generate in new_col_generate:    \n",
    "            new_col_name = col_generate[0]\n",
    "            func = col_generate[1]\n",
    "            data_result[new_col_name] = data_result.apply(lambda row: func(row,data_cols), axis=1)\n",
    "            pick_cols.append(new_col_name)\n",
    "    if col_pick_model is None:\n",
    "        col_pick_model = 'val_loss'\n",
    "#     col_pick_model = 'val_total_loss'\n",
    "    pick_min = col_pick_model_min\n",
    "#     col_pick_model = 'val_accuracy'\n",
    "#     pick_min = False\n",
    "\n",
    "#     summary_history = summarize_history(data_result, pick_cols,col_pick_model=col_pick_model,pick_min = pick_min)\n",
    "#     history_data = load_history_data(data_result, pick_cols)\n",
    "\n",
    "    # modify the increment_fold name manually\n",
    "    data_result['increment_fold'] = data_result['increment_fold'].replace(\n",
    "        ['increment_fold_1', 'increment_fold_2', 'increment_fold_3', 'increment_fold_4'], ['1', '2', '3', '4'])\n",
    "#     summary_history['increment_fold'] = summary_history['increment_fold'].replace(\n",
    "#         ['increment_fold_1', 'increment_fold_2', 'increment_fold_3', 'increment_fold_4'], ['1', '2', '3', '4'])\n",
    "#     history_data['increment_fold'] = history_data['increment_fold'].replace(\n",
    "#         ['increment_fold_1', 'increment_fold_2', 'increment_fold_3', 'increment_fold_4'], ['1', '2', '3', '4'])\n",
    "\n",
    "    return data_result\n",
    "\n",
    "def generate_model_types(row,table_col):\n",
    "    model = row['model']\n",
    "    if 'source_label_space' in table_col:\n",
    "        source_label_spaces = row['source_label_space']\n",
    "    else:\n",
    "        source_label_spaces = 0\n",
    "    if 'target_label_space' in table_col:\n",
    "        target_label_spaces = row['target_label_space']\n",
    "    else:\n",
    "        target_label_spaces = 0\n",
    "\n",
    "#     if model == 'BaseModel'  and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Vanilla_EQ'\n",
    "    if model == 'BaseModel':\n",
    "        return 'Vanilla'\n",
    "#     elif model == 'HeterogeneousModelAdaptation' and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Adapt_EQ'\n",
    "    elif model == 'HeterogeneousModelAdaptation':\n",
    "        return 'Adapt'\n",
    "    elif model == 'ShareLabelModelAdaptation' and (source_label_spaces == target_label_spaces):\n",
    "        return 'AdaptationV2'\n",
    "#     elif model == 'HeterogeneousDANN' and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Dann_EQ'\n",
    "    elif  model == 'HeterogeneousDANN':\n",
    "        return 'Dann'\n",
    "#     elif  model == 'HeterogeneousCDAN'and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Cdan_EQ'\n",
    "    elif  model == 'HeterogeneousCDAN':\n",
    "        return 'Cdan'\n",
    "#     elif model == 'HeterogeneousModelAdaptationDSBN' and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Adapt_DSBN_EQ'\n",
    "    elif model == 'HeterogeneousModelAdaptationDSBN':\n",
    "        return 'Adapt_DSBN'\n",
    "#     elif  model == 'HeterogeneousDANNDSBN' and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Dann_DSBN_EQ'\n",
    "    elif  model == 'HeterogeneousDANNDSBN':\n",
    "        return 'Dann_DSBN'\n",
    "#     elif  model == 'HeterogeneousCDANDSBN' and (source_label_spaces == target_label_spaces):\n",
    "#         return 'Cdan_DSBN_EQ'\n",
    "    elif  model == 'HeterogeneousCDANDSBN':\n",
    "        return 'Cdan_DSBN'\n",
    "    elif model =='HeterogeneousMDD':\n",
    "        return 'Mdd'\n",
    "    elif model =='HeterogeneousDAN':\n",
    "        return 'DAN'\n",
    "    elif model =='HeterogeneousDANDANN':\n",
    "        return 'DAN_DANN'\n",
    "    elif model =='HeterogeneousDANDANNDSBN':\n",
    "        return 'DAN_DANN_DSBN'\n",
    "    elif model =='ShareLabelModelAdaptation':\n",
    "        return 'SHARE_CLA'\n",
    "#     elif model =='FBCNet' and (source_label_spaces == target_label_spaces) :\n",
    "#         return 'FBCNet_EQ'\n",
    "    elif model == 'FBCNet':\n",
    "        return 'FBCNet'\n",
    "    elif model == 'DeepConvNet':\n",
    "        return 'DeepConvNet'\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "# def modify_for_GIGA_Vanilla(row,to_EQ):\n",
    "#     model = row['model']\n",
    "#     dataset = row['dataset']\n",
    "#     current_model_types = row['model_types']\n",
    "#     if model == 'BaseModel' and dataset =='GIGA':\n",
    "#         if to_EQ == True:\n",
    "#             return 'Vanilla_EQ'\n",
    "#         else:\n",
    "#             return 'Vanilla'\n",
    "#     return current_model_types\n",
    "def generate_source_dataset(row,table_col):\n",
    "    model = row['model']\n",
    "    dataset = row['dataset']\n",
    "    if dataset == 'BCI_IV' and model == 'BaseModel':\n",
    "        return 'None'\n",
    "    elif dataset == 'BCI_IV':\n",
    "#         return 'GIGA'\n",
    "        return 'None'\n",
    "\n",
    "    elif dataset == 'BCI_IV_MI':\n",
    "        return 'CLA_2S'\n",
    "    elif dataset == 'BCI_IV_MI_V2':\n",
    "        return 'CLA_HALT_2S'\n",
    "    elif dataset == 'BCI_IV_MI_V3':\n",
    "        return 'CLA_3S'\n",
    "    elif dataset == 'BCI_IV_MI_V4':\n",
    "        return 'HALT'\n",
    "    elif dataset =='BCI_IV_MI_1':\n",
    "        return 'CLA_2S_1'\n",
    "    elif dataset =='BCI_IV_GIGA_MI_V1':\n",
    "        return 'GIGA_CLA_HALT'\n",
    "    elif dataset == 'GIGA':\n",
    "        return 'BCI_IV'\n",
    "#     elif dataset == 'DSBN_BCI_IV_MI':\n",
    "#         return 'DSBN_CLA_2S'\n",
    "    else:\n",
    "        return 'NA'\n",
    "def generate_target_dataset(row,table_col):\n",
    "\n",
    "    model = row['model']\n",
    "    dataset = row['dataset']\n",
    "    if 'source_label_space' in table_col:\n",
    "        source_label_spaces = row['source_label_space']\n",
    "    else:\n",
    "        source_label_spaces = 0\n",
    "    if 'target_label_space' in table_col:\n",
    "        target_label_spaces = row['target_label_space']\n",
    "    else:\n",
    "        target_label_spaces = 0\n",
    "    target_dataset = ''\n",
    "\n",
    "    if dataset =='GIGA':\n",
    "        target_dataset = 'GIGA'\n",
    "    elif 'BCI_IV' in dataset:\n",
    "        target_dataset = 'BCI_IV'\n",
    "    else:\n",
    "        target_dataset='NA'\n",
    "    \n",
    "    if target_label_spaces == 2:\n",
    "        target_dataset = target_dataset+'_2S'\n",
    "    return target_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common path :  C:\\wduong_folder\\Dassl.pytorch-master\\EEG_Dassl_Lightning\\test_2\\eegnet_2\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "prefix lists :  [['vanilla'], ['seed_v0', 'seed_v1', 'seed_v2', 'seed_v3', 'seed_v4', 'seed_v5', 'seed_v6', 'seed_v7', 'seed_v8'], ['norm_none'], ['BCI_IV']]\n",
      "some list full path :  ['C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v0\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v1\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v2\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v3\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v4\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v5\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v6\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v7\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\vanilla\\\\seed_v8\\\\norm_none\\\\BCI_IV_adaptation\\\\transfer_adaptation']\n",
      "data result col  Index(['test_loss', 'test_acc', 'test_fold', 'increment_fold', 'valid_fold',\n",
      "       'dataset', 'normalize', 'alg', 'model', 'counter', 'source_label_space',\n",
      "       'target_label_space', 'seed', 'test_group', 'model_types',\n",
      "       'source_dataset', 'target_dataset'],\n",
      "      dtype='object')\n",
      "                                           test_acc                      \\\n",
      "increment_fold                                    1         2         3   \n",
      "target_dataset source_dataset model_types                                 \n",
      "BCI_IV         None           Vanilla      0.335648  0.403453  0.421682   \n",
      "\n",
      "                                                     \n",
      "increment_fold                                    4  \n",
      "target_dataset source_dataset model_types            \n",
      "BCI_IV         None           Vanilla      0.452257  \n"
     ]
    }
   ],
   "source": [
    "#compare \n",
    "model_list = [\n",
    "    'vanilla',\n",
    "#     'vanilla_equal_label',\n",
    "#     'adaptation',\n",
    "#     'adaptation_DSBN',\n",
    "#     'adapt_dann',\n",
    "#     'adapt_dann_DSBN',\n",
    "#     'adapt_equal_label',\n",
    "#     'adapt_equal_label_DSBN',\n",
    "#     'adapt_equal_dann',\n",
    "#     'adapt_equal_dann_DSBN',\n",
    "#     'adapt_equal_cdan',\n",
    "#     'adapt_equal_cdan_DSBN',\n",
    "    \n",
    "#     'vanilla_aug',\n",
    "#     'vanilla_equal_aug',\n",
    "#     'adapt_aug',\n",
    "#     'adapt_equal_aug',\n",
    "#     'adapt_aug_DSBN',\n",
    "#     'adapt_equal_aug_DSBN',\n",
    "#     'adapt_dann_aug',\n",
    "#     'adapt_equal_dann_aug',\n",
    "#     'adapt_dann_aug_DSBN',\n",
    "#     'adapt_equal_dann_aug_DSBN',\n",
    "#     'adapt_equal_cdan_aug',\n",
    "#     'adapt_equal_cdan_aug_DSBN',\n",
    "]\n",
    "model_data_prefix = [\n",
    "    \"BCI_IV\",\n",
    "#     \"BCI_IV_MI\",\n",
    "#     \"BCI_IV_MI_V2\",\n",
    "]\n",
    "norm_list = ['norm_none']\n",
    "new_col_generate=[\n",
    "                [\"model_types\",generate_model_types],\n",
    "                 [\"source_dataset\",generate_source_dataset],\n",
    "                 [\"target_dataset\",generate_target_dataset]\n",
    "                 ]\n",
    "\n",
    "# \n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\test_2\\\\eegnet_2\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "\n",
    "combine_data={}\n",
    "combine_history={}\n",
    "# field_name = \"aug\"\n",
    "\n",
    "seed_list = [\n",
    "            \"seed_v0\",\n",
    "            \"seed_v1\",\n",
    "            \"seed_v2\",\n",
    "            \"seed_v3\",\n",
    "            \"seed_v4\",\n",
    "            \"seed_v5\",\n",
    "            \"seed_v6\",\n",
    "            \"seed_v7\",\n",
    "            \"seed_v8\",\n",
    "        ]\n",
    "data_result = load_experiment_data(common_path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list,seed_list=seed_list)\n",
    "# aug_case = [\"none\",\"temporal\",\"spatial\",\"T_F\"]\n",
    "# pick_cols = [\"test_group\",\"seed\", \"normalize\", \"dataset\", \"test_fold\", \"increment_fold\", \"valid_fold\", \"history_path\",\"model\"]\n",
    "\n",
    "# for i in range(1,2):\n",
    "#     prefix = 'heterogeneous_adaptation_v'+str(i)\n",
    "#     path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\deepconv\\\\\"+prefix+ \"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# #     path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\braindecode\\\\\"+prefix+ \"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "\n",
    "# #     path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_2\\\\\"+prefix+ \"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "#     print(\"current path : \",path)\n",
    "#     data_result,summary_history,history_data = load_experiment_data(path,model_list,pick_cols=pick_cols,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list,seed_list=seed_list)\n",
    "#     combine_data[aug_case[i-1]] = data_result\n",
    "#     combine_history[aug_case[i-1]] = summary_history\n",
    "# data_result = generate_concat_dataset(combine_data,field_name)\n",
    "# # data_result = data_result.round(decimals=2)\n",
    "# summary_history = generate_concat_dataset(combine_history,field_name)\n",
    "\n",
    "# conditions = [\n",
    "#     [\"target_dataset\",[\"BCI_IV_2S\"]]\n",
    "# ]\n",
    "# data_result_2S = filter_history_information(data_result,conditions)\n",
    "# summary_history_2S = filter_history_information(summary_history,conditions)\n",
    "#view only BCI_IV_2S target dataset\n",
    "# group_format = data_result.groupby([\"test_group\",\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name],as_index=False).mean()\n",
    "# print(group_format)\n",
    "# table = pd.pivot_table(group_format, values=['accuracy','TL_coral_acc'], index=['test_group','target_dataset','source_dataset','model_types',field_name],columns=['increment_fold'])\n",
    "# print(table)\n",
    "print(\"data result col \",data_result.columns)\n",
    "group_format = data_result.groupby([\"test_group\",\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\"],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','source_dataset','model_types'],columns=['increment_fold'])\n",
    "print(table)\n",
    "\n",
    "# save_data_folder = \"update_MI_transfer_learning\\data\"\n",
    "# save_graph_folder = \"update_MI_transfer_learning\\graph\"\n",
    "# output_path = os.path.join(save_data_folder,'Deep_conv_experiment.xlsx')\n",
    "\n",
    "# output_path = os.path.join(save_data_folder,'standard_experiment.xlsx')\n",
    "# table.to_excel(output_path,float_format=\"%.2f\")\n",
    "\n",
    "# print(history_data.columns)\n",
    "# print(\"summary col : \",summary_history.columns)\n",
    "# group_format = summary_history.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name,\"model_choice\"],as_index=False).mean()\n",
    "\n",
    "# table = pd.pivot_table(group_format, values=['test_accuracy'], index=['target_dataset','source_dataset','model_types',field_name,'model_choice'],columns=['increment_fold'])\n",
    "# print(table)\n",
    "\n",
    "# conditions = [\n",
    "#             [\"model_choice\",[\"picked_epoch\"]],\n",
    "# ]\n",
    "\n",
    "# update_summary = filter_history_information(summary_history_2S,conditions)\n",
    "# print(\"update summary\")\n",
    "# group_format = update_summary.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_accuracy'], index=['target_dataset','source_dataset','model_types',field_name],columns=['increment_fold'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1\\share_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "prefix lists :  [['adapt_share_classifier', 'share_dann', 'adapt_share_classifier_aug', 'share_dann_aug'], ['seed_v0', 'seed_v1', 'seed_v2'], ['norm_none'], ['BCI_IV_MI', 'BCI_IV_MI_V2']]\n",
      "some list full path :  []\n",
      "no data path \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-262524e4617d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"current path : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mdata_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistory_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_experiment_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_data_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_data_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_col_generate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_col_generate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnorm_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mcombine_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maug_case\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mcombine_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maug_case\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummary_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-f92f3875953d>\u001b[0m in \u001b[0;36mload_experiment_data\u001b[1;34m(common_path, model_list, seed_list, norm_list, model_data_prefix, pick_cols, col_pick_model, col_pick_model_min, new_col_generate, load_history)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mlist_full_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix_lists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"some list full path : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_full_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mdata_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_full_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_history\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mdata_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-f92f3875953d>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(data_paths, result_folder, result_file_name, info_file_name, load_history)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"the current data path {} does not exist \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mfinal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1\\\\share_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "model_list = [\n",
    "    'adapt_share_classifier',\n",
    "    'share_dann',\n",
    "    \n",
    "    'adapt_share_classifier_aug',\n",
    "    'share_dann_aug'\n",
    "]\n",
    "\n",
    "model_data_prefix = [\n",
    "    \"BCI_IV_MI\",\n",
    "    \"BCI_IV_MI_V2\",\n",
    "]\n",
    "norm_list = ['norm_none',\n",
    "            ]\n",
    "new_col_generate=[\n",
    "                [\"model_types\",generate_model_types],\n",
    "                 [\"source_dataset\",generate_source_dataset],\n",
    "                 [\"target_dataset\",generate_target_dataset]\n",
    "                 ]\n",
    "# data_result,summary_history,history_data = load_experiment_data(common_path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "combine_data={}\n",
    "combine_history={}\n",
    "field_name = \"aug\"\n",
    "aug_case = [\"none\",\"temporal\",\"spatial\",\"T_F\"]\n",
    "for i in range(1,5):\n",
    "    prefix = 'share_adaptation_v'+str(i)\n",
    "    path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1\\\\\"+prefix+ \"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "    print(\"current path : \",path)\n",
    "    data_result,summary_history,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "    combine_data[aug_case[i-1]] = data_result\n",
    "    combine_history[aug_case[i-1]] = summary_history\n",
    "data_result = generate_concat_dataset(combine_data,field_name)\n",
    "summary_history = generate_concat_dataset(combine_history,field_name)\n",
    "\n",
    "# conditions = [\n",
    "#     [\"target_dataset\",[\"BCI_IV_2S\"]]\n",
    "# ]\n",
    "# data_result_2S = filter_history_information(data_result,conditions)\n",
    "# summary_history_2S = filter_history_information(summary_history,conditions)\n",
    "# #view only BCI_IV_2S target dataset\n",
    "group_format = data_result.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['accuracy'], index=['target_dataset','source_dataset','model_types',field_name],columns=['increment_fold'])\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1_1\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1_1\\heterogeneous_adaptation_v2\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1_1\\heterogeneous_adaptation_v3\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1_1\\heterogeneous_adaptation_v4\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "                                                       accuracy             \\\n",
      "increment_fold                                                1          2   \n",
      "target_dataset source_dataset model_types   aug                              \n",
      "BCI_IV         CLA_2S         DAN           T_F       36.564429  40.461034   \n",
      "                                            none      36.839313  41.150656   \n",
      "                                            spatial   35.026042  38.777971   \n",
      "                                            temporal  36.048418  40.644290   \n",
      "                              DAN_DANN      T_F       36.337770  41.044560   \n",
      "                                            none      36.926119  40.837191   \n",
      "                                            spatial   34.659529  38.348765   \n",
      "                                            temporal  35.257523  40.634645   \n",
      "                              DAN_DANN_DSBN T_F       36.829668  41.049383   \n",
      "                                            none      37.099730  41.333912   \n",
      "                                            spatial   36.106289  39.930556   \n",
      "                                            temporal  35.831404  40.890239   \n",
      "               CLA_HALT_2S    DAN           none      36.241319  41.107253   \n",
      "                              DAN_DANN      none      36.607832  41.840278   \n",
      "                              DAN_DANN_DSBN none      36.824846  40.904707   \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F       61.207562  66.415895   \n",
      "                                            none      58.478009  67.187500   \n",
      "                                            spatial   59.008488  65.345293   \n",
      "                                            temporal  62.065972  66.483410   \n",
      "                              DAN_DANN      T_F       61.226852  66.676312   \n",
      "                                            none      58.680556  66.917438   \n",
      "                                            spatial   59.278549  64.583333   \n",
      "                                            temporal  61.979167  67.042824   \n",
      "                              DAN_DANN_DSBN T_F       61.120756  67.071759   \n",
      "                                            none      58.680556  66.068673   \n",
      "                                            spatial   59.683642  65.104167   \n",
      "                                            temporal  61.149691  66.936728   \n",
      "               CLA_HALT_2S    DAN           none      59.866898  66.782407   \n",
      "                              DAN_DANN      none      58.429784  66.859568   \n",
      "                              DAN_DANN_DSBN none      58.998843  67.042824   \n",
      "\n",
      "                                                                 \n",
      "increment_fold                                                3  \n",
      "target_dataset source_dataset model_types   aug                  \n",
      "BCI_IV         CLA_2S         DAN           T_F       39.231289  \n",
      "                                            none      41.521991  \n",
      "                                            spatial   39.197531  \n",
      "                                            temporal  39.496528  \n",
      "                              DAN_DANN      T_F       39.265046  \n",
      "                                            none      41.155478  \n",
      "                                            spatial   39.371142  \n",
      "                                            temporal  38.874421  \n",
      "                              DAN_DANN_DSBN T_F       40.089699  \n",
      "                                            none      42.119985  \n",
      "                                            spatial   39.438657  \n",
      "                                            temporal  40.306713  \n",
      "               CLA_HALT_2S    DAN           none      40.890239  \n",
      "                              DAN_DANN      none      39.650849  \n",
      "                              DAN_DANN_DSBN none      42.047647  \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F       68.180941  \n",
      "                                            none      68.807870  \n",
      "                                            spatial   66.338735  \n",
      "                                            temporal  68.663194  \n",
      "                              DAN_DANN      T_F       67.853009  \n",
      "                                            none      68.701775  \n",
      "                                            spatial   67.698688  \n",
      "                                            temporal  67.959105  \n",
      "                              DAN_DANN_DSBN T_F       67.920525  \n",
      "                                            none      68.161651  \n",
      "                                            spatial   66.984954  \n",
      "                                            temporal  67.631173  \n",
      "               CLA_HALT_2S    DAN           none      66.984954  \n",
      "                              DAN_DANN      none      67.640818  \n",
      "                              DAN_DANN_DSBN none      67.447917  \n",
      "Index(['seed', 'normalize', 'dataset', 'test_fold', 'increment_fold',\n",
      "       'valid_fold', 'history_path', 'model', 'model_types', 'source_dataset',\n",
      "       'target_dataset', 'val_accuracy', 'val_error_rate', 'val_class_0_acc',\n",
      "       'val_class_1_acc', 'val_class_2_acc', 'val_class_3_acc',\n",
      "       'val_total_loss', 'val_loss', 'val_loss_u', 'test_accuracy',\n",
      "       'test_error_rate', 'test_class_0_acc', 'test_class_1_acc',\n",
      "       'test_class_2_acc', 'test_class_3_acc', 'epoch', 'val_loss_d',\n",
      "       'val_lmda_factor'],\n",
      "      dtype='object')\n",
      "summary col :  Index(['seed', 'normalize', 'dataset', 'test_fold', 'increment_fold',\n",
      "       'valid_fold', 'history_path', 'model', 'model_types', 'source_dataset',\n",
      "       'target_dataset', 'model_choice', 'epoch', 'test_accuracy', 'val_loss',\n",
      "       'aug'],\n",
      "      dtype='object')\n",
      "                                                                         test_accuracy  \\\n",
      "increment_fold                                                                       1   \n",
      "target_dataset source_dataset model_types   aug      model_choice                        \n",
      "BCI_IV         CLA_2S         DAN           T_F      best_possible_epoch     38.691165   \n",
      "                                                     picked_epoch            36.564429   \n",
      "                                            none     best_possible_epoch     38.585069   \n",
      "                                                     picked_epoch            36.839313   \n",
      "                                            spatial  best_possible_epoch     37.490355   \n",
      "                                                     picked_epoch            35.026042   \n",
      "                                            temporal best_possible_epoch     38.445216   \n",
      "                                                     picked_epoch            36.048418   \n",
      "                              DAN_DANN      T_F      best_possible_epoch     38.334298   \n",
      "                                                     picked_epoch            36.337770   \n",
      "                                            none     best_possible_epoch     38.826196   \n",
      "                                                     picked_epoch            36.926119   \n",
      "                                            spatial  best_possible_epoch     36.892361   \n",
      "                                                     picked_epoch            34.659529   \n",
      "                                            temporal best_possible_epoch     37.880980   \n",
      "                                                     picked_epoch            35.257523   \n",
      "                              DAN_DANN_DSBN T_F      best_possible_epoch     38.512731   \n",
      "                                                     picked_epoch            36.829668   \n",
      "                                            none     best_possible_epoch     38.903356   \n",
      "                                                     picked_epoch            37.099730   \n",
      "                                            spatial  best_possible_epoch     37.827932   \n",
      "                                                     picked_epoch            36.106289   \n",
      "                                            temporal best_possible_epoch     37.827932   \n",
      "                                                     picked_epoch            35.831404   \n",
      "               CLA_HALT_2S    DAN           none     best_possible_epoch     38.459684   \n",
      "                                                     picked_epoch            36.241319   \n",
      "                              DAN_DANN      none     best_possible_epoch     38.599537   \n",
      "                                                     picked_epoch            36.607832   \n",
      "                              DAN_DANN_DSBN none     best_possible_epoch     38.937114   \n",
      "                                                     picked_epoch            36.824846   \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F      best_possible_epoch     65.297068   \n",
      "                                                     picked_epoch            61.207562   \n",
      "                                            none     best_possible_epoch     62.847222   \n",
      "                                                     picked_epoch            58.478009   \n",
      "                                            spatial  best_possible_epoch     63.281250   \n",
      "                                                     picked_epoch            59.008488   \n",
      "                                            temporal best_possible_epoch     65.412809   \n",
      "                                                     picked_epoch            62.065972   \n",
      "                              DAN_DANN      T_F      best_possible_epoch     64.429012   \n",
      "                                                     picked_epoch            61.226852   \n",
      "                                            none     best_possible_epoch     62.557870   \n",
      "                                                     picked_epoch            58.680556   \n",
      "                                            spatial  best_possible_epoch     62.876157   \n",
      "                                                     picked_epoch            59.278549   \n",
      "                                            temporal best_possible_epoch     65.461034   \n",
      "                                                     picked_epoch            61.979167   \n",
      "                              DAN_DANN_DSBN T_F      best_possible_epoch     65.162037   \n",
      "                                                     picked_epoch            61.120756   \n",
      "                                            none     best_possible_epoch     63.204090   \n",
      "                                                     picked_epoch            58.680556   \n",
      "                                            spatial  best_possible_epoch     62.895448   \n",
      "                                                     picked_epoch            59.683642   \n",
      "                                            temporal best_possible_epoch     65.335648   \n",
      "                                                     picked_epoch            61.149691   \n",
      "               CLA_HALT_2S    DAN           none     best_possible_epoch     62.654321   \n",
      "                                                     picked_epoch            59.866898   \n",
      "                              DAN_DANN      none     best_possible_epoch     62.075617   \n",
      "                                                     picked_epoch            58.429784   \n",
      "                              DAN_DANN_DSBN none     best_possible_epoch     63.261960   \n",
      "                                                     picked_epoch            58.998843   \n",
      "\n",
      "                                                                                     \\\n",
      "increment_fold                                                                    2   \n",
      "target_dataset source_dataset model_types   aug      model_choice                     \n",
      "BCI_IV         CLA_2S         DAN           T_F      best_possible_epoch  42.708333   \n",
      "                                                     picked_epoch         40.461034   \n",
      "                                            none     best_possible_epoch  43.127894   \n",
      "                                                     picked_epoch         41.150656   \n",
      "                                            spatial  best_possible_epoch  41.155478   \n",
      "                                                     picked_epoch         38.777971   \n",
      "                                            temporal best_possible_epoch  43.118248   \n",
      "                                                     picked_epoch         40.644290   \n",
      "                              DAN_DANN      T_F      best_possible_epoch  42.939815   \n",
      "                                                     picked_epoch         41.044560   \n",
      "                                            none     best_possible_epoch  42.703511   \n",
      "                                                     picked_epoch         40.837191   \n",
      "                                            spatial  best_possible_epoch  41.280864   \n",
      "                                                     picked_epoch         38.348765   \n",
      "                                            temporal best_possible_epoch  43.311150   \n",
      "                                                     picked_epoch         40.634645   \n",
      "                              DAN_DANN_DSBN T_F      best_possible_epoch  43.330440   \n",
      "                                                     picked_epoch         41.049383   \n",
      "                                            none     best_possible_epoch  43.774113   \n",
      "                                                     picked_epoch         41.333912   \n",
      "                                            spatial  best_possible_epoch  41.806520   \n",
      "                                                     picked_epoch         39.930556   \n",
      "                                            temporal best_possible_epoch  43.340085   \n",
      "                                                     picked_epoch         40.890239   \n",
      "               CLA_HALT_2S    DAN           none     best_possible_epoch  42.891590   \n",
      "                                                     picked_epoch         41.107253   \n",
      "                              DAN_DANN      none     best_possible_epoch  43.205054   \n",
      "                                                     picked_epoch         41.840278   \n",
      "                              DAN_DANN_DSBN none     best_possible_epoch  43.397955   \n",
      "                                                     picked_epoch         40.904707   \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F      best_possible_epoch  70.736883   \n",
      "                                                     picked_epoch         66.415895   \n",
      "                                            none     best_possible_epoch  69.116512   \n",
      "                                                     picked_epoch         67.187500   \n",
      "                                            spatial  best_possible_epoch  68.229167   \n",
      "                                                     picked_epoch         65.345293   \n",
      "                                            temporal best_possible_epoch  70.003858   \n",
      "                                                     picked_epoch         66.483410   \n",
      "                              DAN_DANN      T_F      best_possible_epoch  69.907407   \n",
      "                                                     picked_epoch         66.676312   \n",
      "                                            none     best_possible_epoch  69.058642   \n",
      "                                                     picked_epoch         66.917438   \n",
      "                                            spatial  best_possible_epoch  68.171296   \n",
      "                                                     picked_epoch         64.583333   \n",
      "                                            temporal best_possible_epoch  70.601852   \n",
      "                                                     picked_epoch         67.042824   \n",
      "                              DAN_DANN_DSBN T_F      best_possible_epoch  70.206404   \n",
      "                                                     picked_epoch         67.071759   \n",
      "                                            none     best_possible_epoch  68.701775   \n",
      "                                                     picked_epoch         66.068673   \n",
      "                                            spatial  best_possible_epoch  68.740355   \n",
      "                                                     picked_epoch         65.104167   \n",
      "                                            temporal best_possible_epoch  70.187114   \n",
      "                                                     picked_epoch         66.936728   \n",
      "               CLA_HALT_2S    DAN           none     best_possible_epoch  69.415509   \n",
      "                                                     picked_epoch         66.782407   \n",
      "                              DAN_DANN      none     best_possible_epoch  69.280478   \n",
      "                                                     picked_epoch         66.859568   \n",
      "                              DAN_DANN_DSBN none     best_possible_epoch  69.203318   \n",
      "                                                     picked_epoch         67.042824   \n",
      "\n",
      "                                                                                     \n",
      "increment_fold                                                                    3  \n",
      "target_dataset source_dataset model_types   aug      model_choice                    \n",
      "BCI_IV         CLA_2S         DAN           T_F      best_possible_epoch  42.076582  \n",
      "                                                     picked_epoch         39.231289  \n",
      "                                            none     best_possible_epoch  43.619792  \n",
      "                                                     picked_epoch         41.521991  \n",
      "                                            spatial  best_possible_epoch  41.608796  \n",
      "                                                     picked_epoch         39.197531  \n",
      "                                            temporal best_possible_epoch  42.023534  \n",
      "                                                     picked_epoch         39.496528  \n",
      "                              DAN_DANN      T_F      best_possible_epoch  42.274306  \n",
      "                                                     picked_epoch         39.265046  \n",
      "                                            none     best_possible_epoch  43.518519  \n",
      "                                                     picked_epoch         41.155478  \n",
      "                                            spatial  best_possible_epoch  41.734182  \n",
      "                                                     picked_epoch         39.371142  \n",
      "                                            temporal best_possible_epoch  41.599151  \n",
      "                                                     picked_epoch         38.874421  \n",
      "                              DAN_DANN_DSBN T_F      best_possible_epoch  42.920525  \n",
      "                                                     picked_epoch         40.089699  \n",
      "                                            none     best_possible_epoch  44.155093  \n",
      "                                                     picked_epoch         42.119985  \n",
      "                                            spatial  best_possible_epoch  42.042824  \n",
      "                                                     picked_epoch         39.438657  \n",
      "                                            temporal best_possible_epoch  42.587770  \n",
      "                                                     picked_epoch         40.306713  \n",
      "               CLA_HALT_2S    DAN           none     best_possible_epoch  43.031443  \n",
      "                                                     picked_epoch         40.890239  \n",
      "                              DAN_DANN      none     best_possible_epoch  43.041088  \n",
      "                                                     picked_epoch         39.650849  \n",
      "                              DAN_DANN_DSBN none     best_possible_epoch  44.102045  \n",
      "                                                     picked_epoch         42.047647  \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F      best_possible_epoch  70.360725  \n",
      "                                                     picked_epoch         68.180941  \n",
      "                                            none     best_possible_epoch  71.383102  \n",
      "                                                     picked_epoch         68.807870  \n",
      "                                            spatial  best_possible_epoch  69.126157  \n",
      "                                                     picked_epoch         66.338735  \n",
      "                                            temporal best_possible_epoch  71.412037  \n",
      "                                                     picked_epoch         68.663194  \n",
      "                              DAN_DANN      T_F      best_possible_epoch  71.055170  \n",
      "                                                     picked_epoch         67.853009  \n",
      "                                            none     best_possible_epoch  71.585648  \n",
      "                                                     picked_epoch         68.701775  \n",
      "                                            spatial  best_possible_epoch  69.483025  \n",
      "                                                     picked_epoch         67.698688  \n",
      "                                            temporal best_possible_epoch  71.604938  \n",
      "                                                     picked_epoch         67.959105  \n",
      "                              DAN_DANN_DSBN T_F      best_possible_epoch  71.884645  \n",
      "                                                     picked_epoch         67.920525  \n",
      "                                            none     best_possible_epoch  70.756173  \n",
      "                                                     picked_epoch         68.161651  \n",
      "                                            spatial  best_possible_epoch  69.251543  \n",
      "                                                     picked_epoch         66.984954  \n",
      "                                            temporal best_possible_epoch  71.199846  \n",
      "                                                     picked_epoch         67.631173  \n",
      "               CLA_HALT_2S    DAN           none     best_possible_epoch  69.097222  \n",
      "                                                     picked_epoch         66.984954  \n",
      "                              DAN_DANN      none     best_possible_epoch  69.897762  \n",
      "                                                     picked_epoch         67.640818  \n",
      "                              DAN_DANN_DSBN none     best_possible_epoch  70.418596  \n",
      "                                                     picked_epoch         67.447917  \n",
      "update summary\n",
      "                                                     test_accuracy             \\\n",
      "increment_fold                                                   1          2   \n",
      "target_dataset source_dataset model_types   aug                                 \n",
      "BCI_IV         CLA_2S         DAN           T_F          36.564429  40.461034   \n",
      "                                            none         36.839313  41.150656   \n",
      "                                            spatial      35.026042  38.777971   \n",
      "                                            temporal     36.048418  40.644290   \n",
      "                              DAN_DANN      T_F          36.337770  41.044560   \n",
      "                                            none         36.926119  40.837191   \n",
      "                                            spatial      34.659529  38.348765   \n",
      "                                            temporal     35.257523  40.634645   \n",
      "                              DAN_DANN_DSBN T_F          36.829668  41.049383   \n",
      "                                            none         37.099730  41.333912   \n",
      "                                            spatial      36.106289  39.930556   \n",
      "                                            temporal     35.831404  40.890239   \n",
      "               CLA_HALT_2S    DAN           none         36.241319  41.107253   \n",
      "                              DAN_DANN      none         36.607832  41.840278   \n",
      "                              DAN_DANN_DSBN none         36.824846  40.904707   \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F          61.207562  66.415895   \n",
      "                                            none         58.478009  67.187500   \n",
      "                                            spatial      59.008488  65.345293   \n",
      "                                            temporal     62.065972  66.483410   \n",
      "                              DAN_DANN      T_F          61.226852  66.676312   \n",
      "                                            none         58.680556  66.917438   \n",
      "                                            spatial      59.278549  64.583333   \n",
      "                                            temporal     61.979167  67.042824   \n",
      "                              DAN_DANN_DSBN T_F          61.120756  67.071759   \n",
      "                                            none         58.680556  66.068673   \n",
      "                                            spatial      59.683642  65.104167   \n",
      "                                            temporal     61.149691  66.936728   \n",
      "               CLA_HALT_2S    DAN           none         59.866898  66.782407   \n",
      "                              DAN_DANN      none         58.429784  66.859568   \n",
      "                              DAN_DANN_DSBN none         58.998843  67.042824   \n",
      "\n",
      "                                                                 \n",
      "increment_fold                                                3  \n",
      "target_dataset source_dataset model_types   aug                  \n",
      "BCI_IV         CLA_2S         DAN           T_F       39.231289  \n",
      "                                            none      41.521991  \n",
      "                                            spatial   39.197531  \n",
      "                                            temporal  39.496528  \n",
      "                              DAN_DANN      T_F       39.265046  \n",
      "                                            none      41.155478  \n",
      "                                            spatial   39.371142  \n",
      "                                            temporal  38.874421  \n",
      "                              DAN_DANN_DSBN T_F       40.089699  \n",
      "                                            none      42.119985  \n",
      "                                            spatial   39.438657  \n",
      "                                            temporal  40.306713  \n",
      "               CLA_HALT_2S    DAN           none      40.890239  \n",
      "                              DAN_DANN      none      39.650849  \n",
      "                              DAN_DANN_DSBN none      42.047647  \n",
      "BCI_IV_2S      CLA_2S         DAN           T_F       68.180941  \n",
      "                                            none      68.807870  \n",
      "                                            spatial   66.338735  \n",
      "                                            temporal  68.663194  \n",
      "                              DAN_DANN      T_F       67.853009  \n",
      "                                            none      68.701775  \n",
      "                                            spatial   67.698688  \n",
      "                                            temporal  67.959105  \n",
      "                              DAN_DANN_DSBN T_F       67.920525  \n",
      "                                            none      68.161651  \n",
      "                                            spatial   66.984954  \n",
      "                                            temporal  67.631173  \n",
      "               CLA_HALT_2S    DAN           none      66.984954  \n",
      "                              DAN_DANN      none      67.640818  \n",
      "                              DAN_DANN_DSBN none      67.447917  \n"
     ]
    }
   ],
   "source": [
    "#compare dan,dan_dann,dan_dann_DSBN for different augment cases\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_2\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\tune_dan\\\\case2\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_1\\\\{}\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "model_list = [\n",
    "    'adapt_dan',\n",
    "    'adapt_equal_dan',\n",
    "    'dan_dann',\n",
    "    'equal_dan_dann',\n",
    "    'dan_dann_DSBN',\n",
    "    'equal_dan_dann_DSBN',\n",
    "    \n",
    "    'adapt_dan_aug',\n",
    "    'adapt_equal_dan_aug',\n",
    "    'dan_dann_aug',\n",
    "    'equal_dan_dann_aug',\n",
    "    'dan_dann_aug_DSBN',\n",
    "    'equal_dan_dann_aug_DSBN',\n",
    "]\n",
    "model_data_prefix = [\n",
    "    \"BCI_IV_MI\",\n",
    "    \"BCI_IV_MI_V2\",\n",
    "]\n",
    "norm_list = ['norm_none']\n",
    "new_col_generate=[\n",
    "                [\"model_types\",generate_model_types],\n",
    "                 [\"source_dataset\",generate_source_dataset],\n",
    "                 [\"target_dataset\",generate_target_dataset]\n",
    "                 ]\n",
    "combine_data={}\n",
    "combine_history={}\n",
    "field_name = \"aug\"\n",
    "# prefix=\"none\"\n",
    "# path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# print(\"current path : \",path)\n",
    "# data_result_1,summary_history_1,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "# path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# print(\"current path : \",path)\n",
    "# data_result_2,summary_history_2,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "\n",
    "# data_result = pd.concat([data_result_1,data_result_2])\n",
    "# data_result = data_result.reset_index(drop=True)\n",
    "# summary_history = pd.concat([summary_history_1,summary_history_2])\n",
    "# summary_history = summary_history.reset_index(drop=True)\n",
    "\n",
    "# combine_data[prefix] = data_result_1\n",
    "# combine_history[prefix] = summary_history_1\n",
    "# aug_case = ['none','temporal','spatial','T_F']\n",
    "\n",
    "# prefix = 'heterogeneous_adaptation_v'+str(1)\n",
    "# path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_2\\\\\"+prefix+ \"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# print(\"current path : \",path)\n",
    "# data_result,summary_history,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "# combine_data[\"5.0\"] = data_result\n",
    "# combine_history[\"5.0\"] = summary_history\n",
    "\n",
    "for i in range(1,5):\n",
    "    prefix = 'heterogeneous_adaptation_v'+str(i)\n",
    "    path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_1\\\\\"+prefix+ \"\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "    print(\"current path : \",path)\n",
    "    data_result,summary_history,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "    combine_data[aug_case[i-1]] = data_result\n",
    "    combine_history[aug_case[i-1]] = summary_history\n",
    "data_result = generate_concat_dataset(combine_data,field_name)\n",
    "summary_history = generate_concat_dataset(combine_history,field_name)\n",
    "\n",
    "group_format = data_result.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name],as_index=False).mean()\n",
    "\n",
    "table = pd.pivot_table(group_format, values=['accuracy'], index=['target_dataset','source_dataset','model_types',field_name],columns=['increment_fold'])\n",
    "print(table)\n",
    "\n",
    "save_data_folder = \"update_MI_transfer_learning\\data\"\n",
    "save_graph_folder = \"update_MI_transfer_learning\\graph\"\n",
    "# output_path = os.path.join(save_data_folder,'tune_EEG_spatial_experiment.xlsx')\n",
    "# table.to_excel(output_path)\n",
    "\n",
    "print(history_data.columns)\n",
    "print(\"summary col : \",summary_history.columns)\n",
    "group_format = summary_history.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name,\"model_choice\"],as_index=False).mean()\n",
    "\n",
    "table = pd.pivot_table(group_format, values=['test_accuracy'], index=['target_dataset','source_dataset','model_types',field_name,'model_choice'],columns=['increment_fold'])\n",
    "print(table)\n",
    "\n",
    "conditions = [\n",
    "            [\"model_choice\",[\"picked_epoch\"]],\n",
    "]\n",
    "\n",
    "update_summary = filter_history_information(summary_history,conditions)\n",
    "print(\"update summary\")\n",
    "group_format = update_summary.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",field_name],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['test_accuracy'], index=['target_dataset','source_dataset','model_types',field_name],columns=['increment_fold'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1_1\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_spatial\\case1\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_spatial\\case2\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_spatial\\case3\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_spatial\\case4\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "test clas col :  []\n",
      "                                                     accuracy             \\\n",
      "increment_fold                                              1          2   \n",
      "target_dataset source_dataset model_types scenario                         \n",
      "BCI_IV_2S      CLA_2S         Adapt_EQ    case0     58.651620  66.792052   \n",
      "                                          case1     56.385031  66.753472   \n",
      "                                          case2     58.371914  65.702160   \n",
      "                                          case3     56.973380  65.316358   \n",
      "                                          case4     56.577932  65.798611   \n",
      "                              DAN_DANN    case0     58.680556  66.917438   \n",
      "                                          case1     56.954090  66.386960   \n",
      "                                          case2     59.905478  65.393519   \n",
      "                                          case3     58.121142  65.528549   \n",
      "                                          case4     54.918981  64.650849   \n",
      "                              Dann_EQ     case0     58.709491  66.801698   \n",
      "                                          case1     57.725694  66.618441   \n",
      "                                          case2     59.394290  64.718364   \n",
      "                                          case3     56.635802  65.046296   \n",
      "                                          case4     54.571759  64.679784   \n",
      "\n",
      "                                                               \n",
      "increment_fold                                              3  \n",
      "target_dataset source_dataset model_types scenario             \n",
      "BCI_IV_2S      CLA_2S         Adapt_EQ    case0     68.152006  \n",
      "                                          case1     67.042824  \n",
      "                                          case2     65.461034  \n",
      "                                          case3     65.364583  \n",
      "                                          case4     67.862654  \n",
      "                              DAN_DANN    case0     68.701775  \n",
      "                                          case1     66.975309  \n",
      "                                          case2     66.676312  \n",
      "                                          case3     65.750386  \n",
      "                                          case4     65.634645  \n",
      "                              Dann_EQ     case0     67.380401  \n",
      "                                          case1     66.946373  \n",
      "                                          case2     67.158565  \n",
      "                                          case3     65.856481  \n",
      "                                          case4     64.998071  \n"
     ]
    }
   ],
   "source": [
    "#compare tuning EEG case study\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_5\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\tune\\\\case1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# model_list = [\n",
    "#     'vanilla',\n",
    "#     'vanilla_equal_label',\n",
    "#     'adaptation',\n",
    "#     'adaptation_DSBN',\n",
    "#     'adapt_dann',\n",
    "#     'adapt_dann_DSBN',\n",
    "# #     'adapt_cdan',\n",
    "#     'adapt_equal_label',\n",
    "#     'adapt_equal_label_DSBN',\n",
    "#     'adapt_equal_dann',\n",
    "#     'adapt_equal_dann_DSBN',\n",
    "\n",
    "# ]\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_2\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\tune_dan\\\\case2\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\tune_spatial\\\\{}\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "model_list = [\n",
    "    'adapt_equal_label',\n",
    "    'adapt_equal_dann',\n",
    "#     'adapt_dan',\n",
    "#     'adapt_equal_dan',\n",
    "#     'dan_dann',\n",
    "    'equal_dan_dann',\n",
    "#     'dan_dann_DSBN',\n",
    "#     'equal_dan_dann_DSBN',\n",
    "]\n",
    "model_data_prefix = [\n",
    "#     \"BCI_IV\",\n",
    "    \"BCI_IV_MI\",\n",
    "#     \"BCI_IV_MI_V2\",\n",
    "]\n",
    "norm_list = ['norm_none']\n",
    "new_col_generate=[\n",
    "                [\"model_types\",generate_model_types],\n",
    "                 [\"source_dataset\",generate_source_dataset],\n",
    "                 [\"target_dataset\",generate_target_dataset]\n",
    "                 ]\n",
    "combine_data={}\n",
    "combine_history={}\n",
    "field_name = \"scenario\"\n",
    "prefix=\"case0\"\n",
    "path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "print(\"current path : \",path)\n",
    "data_result_1,summary_history_1,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "print(\"current path : \",path)\n",
    "data_result_2,summary_history_2,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "\n",
    "data_result = pd.concat([data_result_1,data_result_2])\n",
    "data_result = data_result.reset_index(drop=True)\n",
    "summary_history = pd.concat([summary_history_1,summary_history_2])\n",
    "summary_history = summary_history.reset_index(drop=True)\n",
    "\n",
    "combine_data[prefix] = data_result\n",
    "combine_history[prefix] = summary_history\n",
    "for i in range(1,5):\n",
    "    prefix = 'case'+str(i)\n",
    "    path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\tune_spatial\\\\\"+prefix+ \"\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "    print(\"current path : \",path)\n",
    "    data_result,summary_history,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "    combine_data[prefix] = data_result\n",
    "    combine_history[prefix] = summary_history\n",
    "data_result = generate_concat_dataset(combine_data,field_name)\n",
    "summary_history = generate_concat_dataset(combine_history,field_name)\n",
    "\n",
    "group_format = data_result.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",\"scenario\"],as_index=False).mean()\n",
    "\n",
    "table = pd.pivot_table(group_format, values=['accuracy'], index=['target_dataset','source_dataset','model_types','scenario'],columns=['increment_fold'])\n",
    "print(table)\n",
    "\n",
    "save_data_folder = \"update_MI_transfer_learning\\data\"\n",
    "save_graph_folder = \"update_MI_transfer_learning\\graph\"\n",
    "# output_path = os.path.join(save_data_folder,'tune_EEG_spatial_experiment.xlsx')\n",
    "# table.to_excel(output_path)\n",
    "\n",
    "# print(history_data.columns)\n",
    "# print(\"summary col : \",summary_history.columns)\n",
    "# group_format = summary_history.groupby([\"seed\",\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",\"aug\",\"model_choice\"],as_index=False).mean()\n",
    "\n",
    "# table = pd.pivot_table(group_format, values=['test_accuracy'], index=['target_dataset','source_dataset','model_types','aug','model_choice'],columns=['increment_fold'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\test_mdd\\eegnet_1_1\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_dan\\case1\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_dan\\case2\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_dan\\case3\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "current path :  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.pytorch-master\\tune_dan\\case4\\heterogeneous_adaptation_v1\\{}\\{}\\{}\\{}_adaptation\\transfer_adaptation\n",
      "                                                       accuracy             \\\n",
      "increment_fold                                                1          2   \n",
      "target_dataset source_dataset model_types   scenario                         \n",
      "BCI_IV         CLA_2S         DAN_DANN      case0     36.926119  40.837191   \n",
      "                                            case1     35.908565  41.936728   \n",
      "                                            case2     36.945409  41.502701   \n",
      "                                            case3     35.744599  41.324267   \n",
      "                                            case4     36.222029  41.203704   \n",
      "                              DAN_DANN_DSBN case0     37.099730  41.333912   \n",
      "                                            case1     37.012924  41.039738   \n",
      "                                            case2     36.574074  41.690779   \n",
      "                                            case3     36.525849  41.169946   \n",
      "                                            case4     36.096644  41.203704   \n",
      "               CLA_HALT_2S    DAN_DANN      case0     36.607832  41.840278   \n",
      "                                            case1     36.308835  40.625000   \n",
      "                              DAN_DANN_DSBN case0     36.824846  40.904707   \n",
      "                                            case1     36.279900  41.632909   \n",
      "BCI_IV_2S      CLA_2S         DAN_DANN      case0     58.680556  66.917438   \n",
      "                                            case1     58.227238  66.608796   \n",
      "                                            case2     58.921682  66.377315   \n",
      "                                            case3     59.432870  66.203704   \n",
      "                                            case4     59.645062  66.936728   \n",
      "                              DAN_DANN_DSBN case0     58.680556  66.068673   \n",
      "                                            case1     58.333333  66.502701   \n",
      "                                            case2     58.333333  67.177855   \n",
      "                                            case3     58.989198  65.605710   \n",
      "                                            case4     59.085648  65.895062   \n",
      "               CLA_HALT_2S    DAN_DANN      case0     58.429784  66.859568   \n",
      "                                            case1     60.011574  67.081404   \n",
      "                              DAN_DANN_DSBN case0     58.998843  67.042824   \n",
      "                                            case1     57.629244  67.245370   \n",
      "\n",
      "                                                                 \n",
      "increment_fold                                                3  \n",
      "target_dataset source_dataset model_types   scenario             \n",
      "BCI_IV         CLA_2S         DAN_DANN      case0     41.155478  \n",
      "                                            case1     40.533372  \n",
      "                                            case2     41.010802  \n",
      "                                            case3     40.948110  \n",
      "                                            case4     40.697338  \n",
      "                              DAN_DANN_DSBN case0     42.119985  \n",
      "                                            case1     41.575039  \n",
      "                                            case2     42.081404  \n",
      "                                            case3     42.038002  \n",
      "                                            case4     42.062114  \n",
      "               CLA_HALT_2S    DAN_DANN      case0     39.650849  \n",
      "                                            case1     40.340471  \n",
      "                              DAN_DANN_DSBN case0     42.047647  \n",
      "                                            case1     41.087963  \n",
      "BCI_IV_2S      CLA_2S         DAN_DANN      case0     68.701775  \n",
      "                                            case1     68.856096  \n",
      "                                            case2     67.949460  \n",
      "                                            case3     68.055556  \n",
      "                                            case4     67.920525  \n",
      "                              DAN_DANN_DSBN case0     68.161651  \n",
      "                                            case1     68.248457  \n",
      "                                            case2     67.467207  \n",
      "                                            case3     67.746914  \n",
      "                                            case4     67.988040  \n",
      "               CLA_HALT_2S    DAN_DANN      case0     67.640818  \n",
      "                                            case1     67.746914  \n",
      "                              DAN_DANN_DSBN case0     67.447917  \n",
      "                                            case1     67.534722  \n"
     ]
    }
   ],
   "source": [
    "#compare tuning dan case study\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_5\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\tune\\\\case1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# model_list = [\n",
    "#     'vanilla',\n",
    "#     'vanilla_equal_label',\n",
    "#     'adaptation',\n",
    "#     'adaptation_DSBN',\n",
    "#     'adapt_dann',\n",
    "#     'adapt_dann_DSBN',\n",
    "# #     'adapt_cdan',\n",
    "#     'adapt_equal_label',\n",
    "#     'adapt_equal_label_DSBN',\n",
    "#     'adapt_equal_dann',\n",
    "#     'adapt_equal_dann_DSBN',\n",
    "\n",
    "# ]\n",
    "\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_2\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\tune_dan\\\\case2\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\tune_dan\\\\{}\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "model_list = [\n",
    "#     'adapt_equal_label',\n",
    "#     'adapt_equal_dann',\n",
    "#     'adapt_dan',\n",
    "#     'adapt_equal_dan',\n",
    "    'dan_dann',\n",
    "    'equal_dan_dann',\n",
    "    'dan_dann_DSBN',\n",
    "    'equal_dan_dann_DSBN',\n",
    "]\n",
    "model_data_prefix = [\n",
    "#     \"BCI_IV\",\n",
    "    \"BCI_IV_MI\",\n",
    "    \"BCI_IV_MI_V2\",\n",
    "]\n",
    "norm_list = ['norm_none']\n",
    "new_col_generate=[\n",
    "                [\"model_types\",generate_model_types],\n",
    "                 [\"source_dataset\",generate_source_dataset],\n",
    "                 [\"target_dataset\",generate_target_dataset]\n",
    "                 ]\n",
    "combine_data={}\n",
    "combine_history={}\n",
    "field_name = \"scenario\"\n",
    "prefix=\"case0\"\n",
    "path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1_1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "print(\"current path : \",path)\n",
    "data_result,summary_history,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "# path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\eegnet_1\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# print(\"current path : \",path)\n",
    "# data_result_2,summary_history_2,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "\n",
    "\n",
    "# data_result = pd.concat([data_result_1,data_result_2])\n",
    "# data_result = data_result.reset_index(drop=True)\n",
    "# summary_history = pd.concat([summary_history_1,summary_history_2])\n",
    "# summary_history = summary_history.reset_index(drop=True)\n",
    "\n",
    "combine_data[prefix] = data_result\n",
    "combine_history[prefix] = summary_history\n",
    "for i in range(1,5):\n",
    "    prefix = 'case'+str(i)\n",
    "    path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\tune_dan\\\\\"+prefix+ \"\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "    print(\"current path : \",path)\n",
    "    data_result,summary_history,history_data = load_experiment_data(path,model_list,model_data_prefix = model_data_prefix,new_col_generate=new_col_generate,norm_list=norm_list)\n",
    "    combine_data[prefix] = data_result\n",
    "    combine_history[prefix] = summary_history\n",
    "data_result = generate_concat_dataset(combine_data,field_name)\n",
    "summary_history = generate_concat_dataset(combine_history,field_name)\n",
    "\n",
    "group_format = data_result.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",\"scenario\"],as_index=False).mean()\n",
    "\n",
    "table = pd.pivot_table(group_format, values=['accuracy'], index=['target_dataset','source_dataset','model_types','scenario'],columns=['increment_fold'])\n",
    "print(table)\n",
    "\n",
    "save_data_folder = \"update_MI_transfer_learning\\data\"\n",
    "save_graph_folder = \"update_MI_transfer_learning\\graph\"\n",
    "output_path = os.path.join(save_data_folder,'tune_dan_kernel_experiment.xlsx')\n",
    "# table.to_excel(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_baseline(row,base_line_table):\n",
    "    model_increment = row['increment_fold']\n",
    "    model_accuracy = row['accuracy']\n",
    "#     print(\"model increment : \",model_increment)\n",
    "#     print(\"model acc : \",model_accuracy)\n",
    "    temp = base_line_table[base_line_table['increment_fold'] == model_increment].iloc[0]['accuracy']\n",
    "#     print(\"temp : \",temp)\n",
    "    base_line_accuracy = temp\n",
    "#     print(\"base line acc : \",base_line_accuracy)\n",
    "    diff =  model_accuracy-base_line_accuracy\n",
    "#     print(\"current diff : \",diff)\n",
    "    return diff\n",
    "\n",
    "def plot_relevant_info(data_group):\n",
    "    \n",
    "#     print(\"data group : \",data_group)\n",
    "    conditions = [\n",
    "            [\"model_types\",[\"Vanilla_EQ\"]],\n",
    "        [\"aug\",[\"none\"]]\n",
    "    ]\n",
    "\n",
    "    bench_mark = filter_history_information(data_group,conditions)\n",
    "    data_group['accuracy'] = data_group.apply(lambda row: substract_baseline(row,bench_mark), axis=1)\n",
    "    print(\"update data group\")\n",
    "#     print(\"bench mark : \",bench_mark)\n",
    "#     data_group = filter_history_information(data_group,conditions)\n",
    "    avg_data_group = data_group.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"model_types\",\"aug\"],as_index=False).mean()\n",
    "    print(\"avg group : \",avg_data_group)\n",
    "    table = pd.pivot_table(avg_data_group, values=['accuracy'], index=['target_dataset','source_dataset','model_types','aug'])\n",
    "    print(\"table : \",table)\n",
    "    #view and observe the results for different increment fold for model types\n",
    "#     a = sns.relplot(\n",
    "#         data=avg_data_group,x=\"increment_fold\", y=\"accuracy\", row=\"source_dataset\",col=\"model_types\",\n",
    "#         hue=\"aug\", kind=\"line\", aspect = 0.9\n",
    "#     )\n",
    "#     leg = a._legend\n",
    "#     leg.set_bbox_to_anchor([1.1,0.7])\n",
    "#     a.add_legend()\n",
    "#     a.fig.suptitle(\"Compare avg acc for model types \")\n",
    "#     a.fig.subplots_adjust(top=0.8)\n",
    "\n",
    "    # save_fig_path = os.path.join(save_graph_folder,'study_case_1_avg_acc_model_types.png')\n",
    "    # a.savefig(save_fig_path)\n",
    "\n",
    "#     b = sns.relplot(\n",
    "#         data=avg_data_group,x=\"increment_fold\", y=\"accuracy\", row=\"dataset\",col=\"model_types\",\n",
    "#         hue=\"normalize\", kind=\"line\", aspect = 0.9\n",
    "#     )\n",
    "#     leg = b._legend\n",
    "#     leg.set_bbox_to_anchor([1.1,0.7])\n",
    "#     b.add_legend()\n",
    "#     b.fig.suptitle(\"Compare avg acc for normalization \")\n",
    "#     b.fig.subplots_adjust(top=0.8)\n",
    "#     # save_fig_path = os.path.join(save_graph_folder,'study_case_1_avg_acc_normalize.png')\n",
    "#     # b.savefig(save_fig_path)\n",
    "\n",
    "#     conditions = [\n",
    "#         [\"normalize\",[\"none\"]],\n",
    "#     ]\n",
    "#     data_group = filter_history_information(data_group,conditions)\n",
    "#     c = sns.relplot(\n",
    "#         data=data_group,x=\"increment_fold\", y=\"accuracy\", row=\"dataset\",col=\"seed\",\n",
    "#         hue=\"model_types\", kind=\"line\", aspect = 0.9\n",
    "#     )\n",
    "#     leg = c._legend\n",
    "#     leg.set_bbox_to_anchor([1.1,0.7])\n",
    "#     c.add_legend()\n",
    "#     c.fig.suptitle(\"Compare acc results without normalization \")\n",
    "#     c.fig.subplots_adjust(top=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3fdb2d7e2b97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_relevant_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-a991a63d5e92>\u001b[0m in \u001b[0;36mplot_relevant_info\u001b[1;34m(data_group)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mbench_mark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_history_information\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdata_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msubstract_baseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbench_mark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"update data group\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#     print(\"bench mark : \",bench_mark)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6877\u001b[0m         )\n\u001b[1;32m-> 6878\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6880\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[1;32m--> 296\u001b[1;33m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-a991a63d5e92>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mbench_mark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_history_information\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdata_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msubstract_baseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbench_mark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"update data group\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#     print(\"bench mark : \",bench_mark)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-a991a63d5e92>\u001b[0m in \u001b[0;36msubstract_baseline\u001b[1;34m(row, base_line_table)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(\"model increment : \",model_increment)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(\"model acc : \",model_accuracy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_line_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbase_line_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'increment_fold'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel_increment\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(\"temp : \",temp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbase_line_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2138\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "plot_relevant_info(group_format.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list full path :  ['C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adaptation\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adaptation\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_dann\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_dann\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adaptation\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adaptation\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_dann\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_dann\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adaptation\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adaptation\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_dann\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_dann\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation']\n",
      "test clas col :  []\n",
      "data first      accuracy  error_rate  class_0_acc  class_1_acc  class_2_acc  class_3_acc  \\\n",
      "0  37.673611   62.326389    27.777778    30.787037    17.824074    74.305556   \n",
      "1  45.023148   54.976852    41.203704    31.944444    58.333333    48.611111   \n",
      "2  43.692130   56.307870    31.481481    61.111111    46.064815    36.111111   \n",
      "3  36.226852   63.773148    22.453704    60.185185    38.888889    23.379630   \n",
      "4  41.203704   58.796296    31.712963    55.787037    21.527778    55.787037   \n",
      "\n",
      "     test_fold increment_fold    valid_fold    dataset normalize   alg  \\\n",
      "0  test_fold_1              1  valid_fold_1  BCI_IV_MI      none  none   \n",
      "1  test_fold_1              2  valid_fold_1  BCI_IV_MI      none  none   \n",
      "2  test_fold_1              3  valid_fold_1  BCI_IV_MI      none  none   \n",
      "3  test_fold_2              1  valid_fold_1  BCI_IV_MI      none  none   \n",
      "4  test_fold_2              2  valid_fold_1  BCI_IV_MI      none  none   \n",
      "\n",
      "                          model  counter  source_label_space  \\\n",
      "0  HeterogeneousModelAdaptation  train_x                   2   \n",
      "1  HeterogeneousModelAdaptation  train_x                   2   \n",
      "2  HeterogeneousModelAdaptation  train_x                   2   \n",
      "3  HeterogeneousModelAdaptation  train_x                   2   \n",
      "4  HeterogeneousModelAdaptation  train_x                   2   \n",
      "\n",
      "   target_label_space  seed  \\\n",
      "0                   4     0   \n",
      "1                   4     0   \n",
      "2                   4     0   \n",
      "3                   4     0   \n",
      "4                   4     0   \n",
      "\n",
      "                                        history_path model_types  \n",
      "0  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...       Adapt  \n",
      "1  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...       Adapt  \n",
      "2  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...       Adapt  \n",
      "3  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...       Adapt  \n",
      "4  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...       Adapt  \n",
      "list full path :  ['C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_equal_mdd\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_equal_mdd\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_equal_mdd\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_equal_mdd\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_equal_mdd\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_equal_mdd\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation']\n",
      "test clas col :  []\n",
      "data second      accuracy  error_rate  class_0_acc  class_1_acc    test_fold  \\\n",
      "0  59.606481   40.393519    71.296296    47.916667  test_fold_1   \n",
      "1  64.699074   35.300926    47.685185    81.712963  test_fold_1   \n",
      "2  66.898148   33.101852    50.000000    83.796296  test_fold_1   \n",
      "3  62.847222   37.152778    41.435185    84.259259  test_fold_2   \n",
      "4  62.384259   37.615741    57.638889    67.129630  test_fold_2   \n",
      "\n",
      "  increment_fold    valid_fold    dataset normalize   alg             model  \\\n",
      "0              1  valid_fold_1  BCI_IV_MI      none  none  HeterogeneousMDD   \n",
      "1              2  valid_fold_1  BCI_IV_MI      none  none  HeterogeneousMDD   \n",
      "2              3  valid_fold_1  BCI_IV_MI      none  none  HeterogeneousMDD   \n",
      "3              1  valid_fold_1  BCI_IV_MI      none  none  HeterogeneousMDD   \n",
      "4              2  valid_fold_1  BCI_IV_MI      none  none  HeterogeneousMDD   \n",
      "\n",
      "     DA     counter  source_label_space  target_label_space  seed  \\\n",
      "0  none  bigger_one                   2                   2     0   \n",
      "1  none  bigger_one                   2                   2     0   \n",
      "2  none  bigger_one                   2                   2     0   \n",
      "3  none  bigger_one                   2                   2     0   \n",
      "4  none  bigger_one                   2                   2     0   \n",
      "\n",
      "                                        history_path model_types  \n",
      "0  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...         Mdd  \n",
      "1  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...         Mdd  \n",
      "2  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...         Mdd  \n",
      "3  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...         Mdd  \n",
      "4  C:\\wduong_folder\\Dassl.pytorch-master\\Dassl.py...         Mdd  \n",
      "list full path :  ['C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_cdan_aug\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_cdan_aug\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_cdan_aug_DSBN\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v0\\\\adapt_cdan_aug_DSBN\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_cdan_aug\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_cdan_aug\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_cdan_aug_DSBN\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v1\\\\adapt_cdan_aug_DSBN\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_cdan_aug\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_cdan_aug\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_cdan_aug_DSBN\\\\norm_none\\\\BCI_IV_MI_adaptation\\\\transfer_adaptation', 'C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\seed_v2\\\\adapt_cdan_aug_DSBN\\\\norm_none\\\\BCI_IV_MI_V2_adaptation\\\\transfer_adaptation']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test clas col :  []\n",
      "                                                     accuracy             \\\n",
      "increment_fold                                              1          2   \n",
      "target_dataset source_dataset model_types aug                              \n",
      "BCI_IV         CLA_2S         Adapt       none      36.279900  41.169946   \n",
      "                              Cdan        temporal  36.424576  40.668403   \n",
      "                              Cdan_DSBN   temporal  36.492091  41.396605   \n",
      "                              Dann        none      36.029128  40.581597   \n",
      "                              Mdd         none      61.429398  64.882330   \n",
      "               CLA_HALT_2S    Adapt       none      35.831404  41.285687   \n",
      "                              Cdan        temporal  35.971258  40.557485   \n",
      "                              Cdan_DSBN   temporal  36.467978  40.803434   \n",
      "                              Dann        none      36.993634  40.678048   \n",
      "                              Mdd         none      61.429398  65.837191   \n",
      "\n",
      "                                                               \n",
      "increment_fold                                              3  \n",
      "target_dataset source_dataset model_types aug                  \n",
      "BCI_IV         CLA_2S         Adapt       none      41.603974  \n",
      "                              Cdan        temporal  40.605710  \n",
      "                              Cdan_DSBN   temporal  40.393519  \n",
      "                              Dann        none      40.899884  \n",
      "                              Mdd         none      66.435185  \n",
      "               CLA_HALT_2S    Adapt       none      41.845100  \n",
      "                              Cdan        temporal  39.781057  \n",
      "                              Cdan_DSBN   temporal  40.383873  \n",
      "                              Dann        none      39.163773  \n",
      "                              Mdd         none      66.165123  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "model_list = [\n",
    "#     'vanilla',\n",
    "#     'vanilla_equal_label',\n",
    "    'adaptation',\n",
    "    'adaptation_DSBN',\n",
    "    'adapt_dann',\n",
    "    'adapt_dann_DSBN'\n",
    "#     'adapt_cdan',\n",
    "#     'adapt_equal_label',\n",
    "#     'adapt_equal_dann',\n",
    "]\n",
    "model_data_prefix = [\n",
    "#     \"BCI_IV\",\"GIGA\",\n",
    "    \"BCI_IV_MI\",\n",
    "    \"BCI_IV_MI_V2\",\n",
    "#     \"BCI_IV_MI_V3\",\n",
    "#     \"BCI_IV_GIGA_MI\",\n",
    "#     \"BCI_IV_GIGA_MI_V1\"\n",
    "]\n",
    "norm_list = ['norm_none',\n",
    "#              'norm_zscore',\n",
    "#              'norm_zscore_1'\n",
    "            ]\n",
    "\n",
    "data_result,summary_history,history_data = load_experiment_data(common_path,model_list,model_data_prefix = model_data_prefix,new_col_generate=[\"model_types\",generate_model_types],norm_list=norm_list)\n",
    "\n",
    "print(\"data first \",data_result.head())\n",
    "\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\mdd\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "\n",
    "model_list = [\n",
    "#     'vanilla',\n",
    "#     'vanilla_equal_label',\n",
    "#     'adaptation',\n",
    "#     'adapt_dann',\n",
    "#     'adapt_cdan',\n",
    "#     'adapt_cdan_DSBN',\n",
    "#     'adapt_equal_cdan',\n",
    "#     'adapt_equal_cdan_DSBN'\n",
    "\n",
    "#     'adapt_equal_label',\n",
    "#     'adapt_equal_dann',\n",
    "    'adapt_equal_mdd'\n",
    "]\n",
    "model_data_prefix = [\n",
    "#     \"BCI_IV\",\n",
    "    \"BCI_IV_MI\",\n",
    "    \"BCI_IV_MI_V2\",\n",
    "\n",
    "]\n",
    "norm_list = ['norm_none',\n",
    "#              'norm_zscore',\n",
    "#              'norm_zscore_1'\n",
    "            ]\n",
    "\n",
    "data_result_1,summary_history_1,history_data_1 = load_experiment_data(common_path,model_list,model_data_prefix = model_data_prefix,new_col_generate=[\"model_types\",generate_model_types],norm_list=norm_list)\n",
    "\n",
    "print(\"data second \",data_result_1.head())\n",
    "\n",
    "\n",
    "data_result_1 = pd.concat([data_result_1,data_result])\n",
    "data_result_1 = data_result_1.reset_index(drop=True)\n",
    "summary_history_1 = pd.concat([summary_history_1,summary_history])\n",
    "summary_history_1 = summary_history_1.reset_index(drop=True)\n",
    "history_data_1 = pd.concat([history_data_1,history_data])\n",
    "history_data_1 = history_data_1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#2 temporal augmentation exp\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDA\\\\heterogeneous_adaptation_v1\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "model_list = [\n",
    "#     'vanilla_aug',\n",
    "#     'adapt_aug',\n",
    "#     'adapt_equal_aug',\n",
    "#     'vanilla_equal_aug',\n",
    "#     'adapt_equal_dann_aug',\n",
    "#     'adapt_equal_cdan_aug',\n",
    "    'adapt_cdan_aug',\n",
    "\n",
    "#     'adapt_aug_DSBN',\n",
    "#     'adapt_dann_aug_DSBN',\n",
    "#     'adapt_equal_aug_DSBN',\n",
    "#     'adapt_equal_dann_aug_DSBN',\n",
    "#     'adapt_equal_cdan_aug_DSBN',\n",
    "        'adapt_cdan_aug_DSBN',\n",
    "\n",
    "\n",
    "]\n",
    "model_data_prefix = [\"BCI_IV_MI\",\"BCI_IV_MI_V2\"]\n",
    "\n",
    "data_result_1_1,summary_history_1_1,history_data_1_1 = load_experiment_data(common_path,model_list,model_data_prefix = model_data_prefix,new_col_generate=[\"model_types\",generate_model_types])\n",
    "\n",
    "#3 T_F temporal augmentation exp\n",
    "# common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\Dassl.pytorch-master\\\\test_mdd\\\\CDAN_1\\\\heterogeneous_adaptation_v3\\\\{}\\\\{}\\\\{}\\\\{}_adaptation\\\\transfer_adaptation\"\n",
    "# model_list = [\n",
    "#     'vanilla_aug',\n",
    "#     'adapt_aug',\n",
    "#     'adapt_equal_aug',\n",
    "#     'vanilla_equal_aug',\n",
    "#     'adapt_equal_dann_aug',\n",
    "#     'adapt_equal_cdan_aug',\n",
    "#     'adapt_aug_DSBN',\n",
    "#     'adapt_dann_aug_DSBN',\n",
    "#     'adapt_equal_aug_DSBN',\n",
    "#     'adapt_equal_dann_aug_DSBN',\n",
    "#     'adapt_equal_cdan_aug_DSBN',\n",
    "\n",
    "# ]\n",
    "# model_data_prefix = [\"BCI_IV\",\"BCI_IV_MI\",\"BCI_IV_MI_V2\"]\n",
    "\n",
    "# data_result_1_2,summary_history_1_2,history_data_1_2 = load_experiment_data(common_path,model_list,model_data_prefix = model_data_prefix,new_col_generate=[\"model_types\",generate_model_types])\n",
    "\n",
    "# data_result_1 = pd.concat([data_result_1,data_result_1_1])\n",
    "# data_result_1 = data_result_1.reset_index(drop=True)\n",
    "# summary_history_1 = pd.concat([summary_history_1,summary_history_1_1])\n",
    "# summary_history_1 = summary_history_1.reset_index(drop=True)\n",
    "# history_data_1 = pd.concat([history_data_1,history_data_1_1])\n",
    "# history_data_1 = history_data_1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# combine_data = {\n",
    "#     'none':data_result,\n",
    "#     'avg_d_loss':data_result_1,\n",
    "# }\n",
    "# field_name = \"adv_loss\"\n",
    "\n",
    "combine_data = {\n",
    "    'none':data_result_1,\n",
    "    'temporal':data_result_1_1,\n",
    "#     'T_F':data_result_1_2\n",
    "}\n",
    "field_name = \"aug\"\n",
    "data_result = generate_concat_dataset(combine_data,field_name)\n",
    "# combine_data = {\n",
    "#     'none':summary_history,\n",
    "#     'avg_d_loss':summary_history_1,\n",
    "# }\n",
    "# field_name = \"adv_loss\"\n",
    "\n",
    "combine_data = {\n",
    "    'none':summary_history_1,\n",
    "    'temporal':summary_history_1_1,\n",
    "#     'T_F':summary_history_1_2\n",
    "}\n",
    "field_name = \"aug\"\n",
    "summary_history = generate_concat_dataset(combine_data,field_name)\n",
    "data_result[\"source_dataset\"] = data_result.apply(lambda row: generate_source_dataset(row), axis=1)\n",
    "data_result[\"target_dataset\"] = data_result.apply(lambda row: generate_target_dataset(row), axis=1)\n",
    "summary_history[\"source_dataset\"] = summary_history.apply(lambda row: generate_source_dataset(row), axis=1)\n",
    "summary_history[\"target_dataset\"] = summary_history.apply(lambda row: generate_target_dataset(row), axis=1)\n",
    "# conditions = [\n",
    "#     [\"seed\",[0]],\n",
    "# ]\n",
    "# #HLB = heterogeneous label spaces\n",
    "# data_result = filter_history_information(data_result,conditions)\n",
    "\n",
    "# data_result[\"model_types\"] = data_result.apply(lambda row: modify_for_GIGA_Vanilla(row,False), axis=1)\n",
    "# group_format = data_result.groupby([\"seed\",\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",\"aug\"],as_index=False).mean()\n",
    "group_format = data_result.groupby([\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",\"aug\"],as_index=False).mean()\n",
    "\n",
    "table = pd.pivot_table(group_format, values=['accuracy'], index=['target_dataset','source_dataset','model_types','aug'],columns=['increment_fold'])\n",
    "print(table)\n",
    "\n",
    "# print(history_data.columns)\n",
    "# print(\"summary col : \",summary_history.columns)\n",
    "# group_format = summary_history.groupby([\"seed\",\"normalize\",\"target_dataset\",\"source_dataset\",\"increment_fold\",\"model_types\",\"aug\",\"model_choice\"],as_index=False).mean()\n",
    "\n",
    "# table = pd.pivot_table(group_format, values=['test_accuracy'], index=['target_dataset','source_dataset','model_types','aug','model_choice'],columns=['increment_fold'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

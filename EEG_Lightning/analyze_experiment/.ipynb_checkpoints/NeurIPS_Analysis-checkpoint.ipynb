{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3d23c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from util.util import generate_data_paths,generate_history_results_path, load_history_data, generate_concat_dataset,filter_history_information,load_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f17d71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_history(data_table, pick_cols, col_pick_model=\"val_loss\", pick_min=True, max_epochs=100,min_epoch=10,\n",
    "                      col_pick_max=\"test_acc\", data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        fix_col_pick_model = col_pick_model\n",
    "        history_data = pd.read_csv(path)\n",
    "        available_cols = history_data.columns\n",
    "  \n",
    "        # check if col_pick_model exist\n",
    "        if not col_pick_model in history_data.columns:\n",
    "            print(\"col {} isn't in the history data \".format(col_pick_model))\n",
    "            print(\"use default val_loss as pick col\")\n",
    "            fix_col_pick_model = \"val_loss\"\n",
    "        \n",
    "\n",
    "        # limit total epoch to max epoch\n",
    "        history_epoch = len(history_data)\n",
    "        if history_epoch > max_epochs:\n",
    "            history_data = history_data[:max_epochs]\n",
    "        \n",
    "        if history_epoch > min_epoch:\n",
    "            history_data = history_data[min_epoch:]\n",
    "#             print(\"update history data :\",history_data.head())\n",
    "            history_data = history_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # deal with how to use a metric to pick best model\n",
    "        if pick_min:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmin()\n",
    "        else:\n",
    "            pick_row_idx = history_data[fix_col_pick_model].argmax()\n",
    "\n",
    "        #\n",
    "\n",
    "        # val_loss_name = 'val_loss' if 'val_loss' in history_data.columns else 'val_loss_x'\n",
    "        metric_pick_model = ['']\n",
    "        # get max possible test_auc score information\n",
    "        best_row_idx = history_data[col_pick_max].argmax()\n",
    "\n",
    "#         print(\"pick row idx : \",pick_row_idx)\n",
    "#         print(\"pick best idx : \",best_row_idx)\n",
    "        \n",
    "        # best_col_pick_model = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # best_col_pick_max = history_data.loc[best_row_idx, col_pick_max]\n",
    "        # \n",
    "        # best_test_auc = history_data.loc[best_row_idx, col_pick_max]\n",
    "        test_class_col = [col for col in pick_cols if \"test_class_\" in col]\n",
    "\n",
    "        history_info_dict = {\n",
    "            \"model_choice\": [\"best_possible_epoch\", \"picked_epoch\"],\n",
    "            \"epoch\": [best_row_idx, pick_row_idx],\n",
    "            col_pick_max: [history_data.loc[best_row_idx, col_pick_max], history_data.loc[pick_row_idx, col_pick_max]],\n",
    "            fix_col_pick_model: [history_data.loc[best_row_idx, fix_col_pick_model],\n",
    "                                 history_data.loc[pick_row_idx, fix_col_pick_model]],\n",
    "            \"history_path\": [path, path]\n",
    "        }\n",
    "\n",
    "        history_information = pd.DataFrame(history_info_dict)\n",
    "        history_information_table.append(history_information)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def generate_history_results_path(row, full_result_path):\n",
    "    remain='default\\\\version_0\\\\metrics.csv'\n",
    "    test_fold = row['test_fold']\n",
    "    shuffle_fold = row['shuffle_fold']\n",
    "    increment_fold = row['increment_fold']\n",
    "    valid_fold = row['valid_fold']\n",
    "    history_path = os.path.join(full_result_path,test_fold, shuffle_fold,increment_fold, valid_fold,\n",
    "                                remain)\n",
    "#     print(\"current history path : \",history_path)\n",
    "    return history_path\n",
    "def load_history_data(data_table, pick_cols, data_path_col='history_path'):\n",
    "    if data_path_col not in data_table.columns:\n",
    "        print(\"there are no history path to load history data\")\n",
    "        return\n",
    "    history_information_table = []\n",
    "    temp = data_table[pick_cols]\n",
    "    print(\"temp col : \",temp.columns)\n",
    "    history_cols = temp[data_path_col]\n",
    "    for path in history_cols.values:\n",
    "        history_data = pd.read_csv(path)\n",
    "        history_data[data_path_col] = [path] * len(history_data)\n",
    "        history_information_table.append(history_data)\n",
    "    history_information_table = pd.concat(history_information_table)\n",
    "    merge_table = pd.merge(temp, history_information_table, on=[data_path_col])\n",
    "    return merge_table\n",
    "def load_data(data_paths, result_folder, result_file_name, info_file_name, load_history=False):\n",
    "    list_data = []\n",
    "    if len(data_paths) ==0:\n",
    "        print(\"no data path \")\n",
    "    for data_path in data_paths:\n",
    "        result_folder_path = os.path.join(data_path, result_folder)\n",
    "        result_data_path = os.path.join(result_folder_path, result_file_name)\n",
    "        # check if file result exists\n",
    "        if os.path.exists(result_data_path):\n",
    "            data = pd.read_excel(result_data_path)\n",
    "            data_size = len(data)\n",
    "            info_data_path = os.path.join(result_folder_path, info_file_name)\n",
    "            if os.path.exists(info_data_path):\n",
    "                with open(info_data_path) as f:\n",
    "                    info_data = json.load(f)\n",
    "                    extra_fields = info_data[\"EXTRA_FIELDS\"]\n",
    "                    field_names = list(extra_fields.keys())\n",
    "                    for field_name in field_names:\n",
    "                        if extra_fields[field_name] == []:\n",
    "                            extra_fields[field_name] = None\n",
    "                        data[field_name] = data_size*[extra_fields[field_name]]\n",
    "                list_data.append(data)\n",
    "            else:\n",
    "                print(\"no data info for {} \".format(result_data_path))\n",
    "\n",
    "            if load_history:\n",
    "                data['history_path'] = data.apply(lambda row: generate_history_results_path(row, data_path), axis=1)\n",
    "#                 print(\"load current history path : \",data['history_path'].values[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"the current data path {} does not exist \".format(result_data_path))\n",
    "\n",
    "    final_data = pd.concat(list_data).reset_index(drop=True)\n",
    "    return final_data\n",
    "# prefix_lists=[augmentation_prefix,norm_prefix,model_prefix,dataset_prefix]\n",
    "def load_experiment_data(common_path, prefix_lists=None,pick_cols=None,\n",
    "                         col_pick_model=None,col_pick_model_min=True,\n",
    "                         new_col_generate=None,load_history = False):\n",
    "\n",
    "    result_folder = 'result_folder'\n",
    "    file_name = 'model_result.xlsx'\n",
    "    info_file_name = 'model_info.json'\n",
    "\n",
    "    list_full_path = generate_data_paths(common_path, prefix_lists, [])\n",
    "    data_result = load_data(list_full_path, result_folder, file_name, info_file_name, load_history=load_history)\n",
    "    data_cols = data_result.columns\n",
    "\n",
    "    pick_cols = ['test_fold', 'shuffle_fold', 'increment_fold',\n",
    "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
    "       'model', 'source_label_space', 'target_label_space','history_path']\n",
    "    if pick_cols is None:\n",
    "        pick_cols = list(data_cols)\n",
    "\n",
    "#     if new_col_generate is not None:\n",
    "#         for col_generate in new_col_generate:    \n",
    "#             new_col_name = col_generate[0]\n",
    "#             func = col_generate[1]\n",
    "#             data_result[new_col_name] = data_result.apply(lambda row: func(row,data_cols), axis=1)\n",
    "#             pick_cols.append(new_col_name)\n",
    "    if col_pick_model is None:\n",
    "        col_pick_model = 'val_loss'\n",
    "    pick_min = col_pick_model_min\n",
    "    print(\"data result cols : \",data_result.columns)\n",
    "    if load_history:\n",
    "        summary = summarize_history(data_result, pick_cols)\n",
    "        history_data = load_history_data(data_result, pick_cols)\n",
    "        return data_result,history_data,summary\n",
    "    return data_result\n",
    "\n",
    "\n",
    "\n",
    "def modify_col_info(data_result):\n",
    "    data_result['increment_fold'] = data_result['increment_fold'].replace(\n",
    "    ['increment_fold_1', 'increment_fold_2', 'increment_fold_3'], ['1', '2', '3'])\n",
    "    data_result['valid_fold'] = data_result['valid_fold'].replace(\n",
    "        ['valid_fold_1', 'valid_fold_2', 'valid_fold_3'], ['1', '2', '3'])\n",
    "    data_result['shuffle_fold'] = data_result['shuffle_fold'].replace(\n",
    "        ['shuffle_fold_1', 'shuffle_fold_2', 'shuffle_fold_3', 'shuffle_fold_4'], ['1', '2', '3','4'])\n",
    "    data_result['test_fold'] = data_result['test_fold'].replace(\n",
    "        ['test_fold_1'], ['1'])\n",
    "    data_result['aug'] = data_result['aug'].replace(\n",
    "        ['no_aug', 'temporal_aug'], ['no', 'temp'])\n",
    "    data_result['normalize'] = data_result['normalize'].replace(\n",
    "        ['chan_norm', 'no_norm'], ['chan', 'no'])\n",
    "    data_result['model'] = data_result['model'].replace(\n",
    "        ['ComponentAdaptation', 'BaseModel', 'MultiDatasetAdaptation'], ['component', 'base','adapt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f03d7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare \n",
    "model_list_prefix = [\n",
    "    'vanilla',\n",
    "    'adaptation',\n",
    "    'component_adapt'\n",
    "]\n",
    "target_dataset_list_prefix = [\n",
    "    \"BCI_IV\",\n",
    "    \"Cho2017\",\n",
    "    \"Physionet\"\n",
    "]\n",
    "augmentation_list_prefix = [\n",
    "    'no_aug',\n",
    "    'temp_aug'\n",
    "]\n",
    "norm_list_prefix = [\n",
    "    'no_norm',\n",
    "    'chan_norm'\n",
    "]\n",
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_1\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "10b985a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'shuffle_fold', 'increment_fold',\n",
      "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
      "       'model', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_result_1 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "# data_result_1,progress_data_1,summary_1 = load_experiment_data(common_path,prefix_lists=prefix_list,load_history=True)\n",
    "modify_col_info(data_result_1)\n",
    "# modify_col_info(progress_data_1)\n",
    "# modify_col_info(summary_1)\n",
    "\n",
    "# summary_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1faf4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_data_1.head()\n",
    "# conditions = [\n",
    "#     [\"target_dataset\",[\"BCI_IV_2S\"]]\n",
    "# ]\n",
    "# def modify_history_data(history_data):\n",
    "#     model = history_data[\"model\"]\n",
    "# #only pick between epoch 10-30 for adapt model\n",
    "# adapt_data = progress_data_1[progress_data_1['model'].isin([\"adapt\",\"component\"])]\n",
    "# adapt_data = adapt_data[adapt_data['epoch']>10]\n",
    "# # filter_history_information(progress_data_1,)\n",
    "# print(adapt_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "296ed560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             test_acc  \\\n",
      "increment_fold                                                      1   \n",
      "target_dataset normalize aug  model_choice        model                 \n",
      "BCI_IV         chan      no   best_possible_epoch adapt      0.713078   \n",
      "                                                  base       0.634488   \n",
      "                                                  component  0.635210   \n",
      "                              picked_epoch        adapt      0.696782   \n",
      "                                                  base       0.630466   \n",
      "...                                                               ...   \n",
      "physionet      no        temp best_possible_epoch base       0.409583   \n",
      "                                                  component  0.412248   \n",
      "                              picked_epoch        adapt      0.392486   \n",
      "                                                  base       0.365903   \n",
      "                                                  component  0.384084   \n",
      "\n",
      "                                                                       \\\n",
      "increment_fold                                                      2   \n",
      "target_dataset normalize aug  model_choice        model                 \n",
      "BCI_IV         chan      no   best_possible_epoch adapt      0.664810   \n",
      "                                                  base       0.622422   \n",
      "                                                  component  0.621236   \n",
      "                              picked_epoch        adapt      0.647896   \n",
      "                                                  base       0.618657   \n",
      "...                                                               ...   \n",
      "physionet      no        temp best_possible_epoch base       0.394059   \n",
      "                                                  component  0.412912   \n",
      "                              picked_epoch        adapt      0.378538   \n",
      "                                                  base       0.360327   \n",
      "                                                  component  0.385172   \n",
      "\n",
      "                                                                       \n",
      "increment_fold                                                      3  \n",
      "target_dataset normalize aug  model_choice        model                \n",
      "BCI_IV         chan      no   best_possible_epoch adapt      0.640505  \n",
      "                                                  base       0.606229  \n",
      "                                                  component  0.591206  \n",
      "                              picked_epoch        adapt      0.625859  \n",
      "                                                  base       0.604407  \n",
      "...                                                               ...  \n",
      "physionet      no        temp best_possible_epoch base       0.425772  \n",
      "                                                  component  0.436201  \n",
      "                              picked_epoch        adapt      0.398788  \n",
      "                                                  base       0.392582  \n",
      "                                                  component  0.413875  \n",
      "\n",
      "[72 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(\"data result col \",data_result.columns)\n",
    "# print(\"unique aug : \",np.unique(data_result['aug']))\n",
    "# group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"increment_fold\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['increment_fold'])\n",
    "# print(table)\n",
    "\n",
    "# group_format = summary_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"increment_fold\",\"model_choice\",\"model\"],as_index=False).mean()\n",
    "# table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug',\"model_choice\",'model'],columns=['increment_fold'])\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "08689be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data result cols :  Index(['test_acc', 'test_loss', 'test_fold', 'shuffle_fold', 'increment_fold',\n",
      "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
      "       'model', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "data result col  Index(['test_acc', 'test_loss', 'test_fold', 'shuffle_fold', 'increment_fold',\n",
      "       'valid_fold', 'target_dataset', 'source_dataset', 'normalize', 'aug',\n",
      "       'model', 'source_label_space', 'target_label_space'],\n",
      "      dtype='object')\n",
      "unique aug :  ['no' 'temp']\n",
      "                                         test_acc                    \n",
      "increment_fold                                  1         2         3\n",
      "target_dataset normalize aug  model                                  \n",
      "BCI_IV         chan      no   adapt      0.713903  0.638047  0.637376\n",
      "                              base       0.669864  0.642636  0.614171\n",
      "                              component  0.578177  0.558684  0.455858\n",
      "                         temp adapt      0.742574  0.666615  0.658313\n",
      "                              base       0.738036  0.700392  0.663504\n",
      "                              component  0.658313  0.548577  0.514783\n",
      "               no        no   adapt      0.703383  0.654806  0.639198\n",
      "                              base       0.639748  0.605972  0.576423\n",
      "                              component  0.696988  0.619792  0.560197\n",
      "                         temp adapt      0.716172  0.652279  0.592203\n",
      "                              base       0.706374  0.658571  0.630776\n",
      "                              component  0.645111  0.612005  0.562362\n",
      "cho2017        chan      no   adapt      0.598402  0.605049  0.616988\n",
      "                              base       0.568306  0.583019  0.622503\n",
      "                              component  0.569414  0.588406  0.608865\n",
      "                         temp adapt      0.595884  0.608929  0.622097\n",
      "                              base       0.580884  0.584748  0.633856\n",
      "                              component  0.578750  0.591126  0.598916\n",
      "               no        no   adapt      0.606154  0.612588  0.627595\n",
      "                              base       0.571429  0.599747  0.632353\n",
      "                              component  0.587734  0.566615  0.583446\n",
      "                         temp adapt      0.602212  0.607996  0.639525\n",
      "                              base       0.572720  0.588641  0.633930\n",
      "                              component  0.582935  0.559301  0.571774\n",
      "physionet      chan      no   adapt      0.387186  0.370533  0.389173\n",
      "                              base       0.350428  0.331564  0.356297\n",
      "                              component  0.373976  0.378921  0.408690\n",
      "                         temp adapt      0.388159  0.358468  0.398053\n",
      "                              base       0.349701  0.337049  0.344425\n",
      "                              component  0.380383  0.386785  0.412997\n",
      "               no        no   adapt      0.382000  0.370636  0.393685\n",
      "                              base       0.342633  0.341669  0.345738\n",
      "                              component  0.371561  0.370453  0.399970\n",
      "                         temp adapt      0.385574  0.367401  0.381754\n",
      "                              base       0.357524  0.348546  0.360707\n",
      "                              component  0.381565  0.380032  0.395019\n"
     ]
    }
   ],
   "source": [
    "prefix_list = [augmentation_list_prefix,norm_list_prefix,model_list_prefix,target_dataset_list_prefix]\n",
    "common_path = \"C:\\\\wduong_folder\\\\Dassl.pytorch-master\\\\EEG_Dassl_Lightning\\\\NeurIPS_competition\\\\experiment_2\\\\{}\\\\{}\\\\{}\\\\{}\\\\model\"\n",
    "\n",
    "data_result_2 = load_experiment_data(common_path,prefix_lists=prefix_list)\n",
    "\n",
    "modify_col_info(data_result_2)\n",
    "\n",
    "print(\"data result col \",data_result_2.columns)\n",
    "print(\"unique aug : \",np.unique(data_result_2['aug']))\n",
    "group_format = data_result_2.groupby([\"normalize\",\"aug\",\"target_dataset\",\"increment_fold\",\"model\"],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['increment_fold'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d6dd3ef5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         test_acc                    \n",
      "increment_fold                                  1         2         3\n",
      "target_dataset normalize aug  model                                  \n",
      "BCI_IV         chan      no   adapt      0.697814  0.647896  0.625859\n",
      "                              base       0.630466  0.618657  0.604407\n",
      "                              component  0.630466  0.605404  0.574876\n",
      "                         temp adapt      0.732364  0.673938  0.649821\n",
      "                              base       0.728960  0.675485  0.656319\n",
      "                              component  0.639645  0.640058  0.573398\n",
      "               no        no   adapt      0.700804  0.623350  0.613139\n",
      "                              base       0.637067  0.595245  0.574979\n",
      "                              component  0.596947  0.570390  0.447951\n",
      "                         temp adapt      0.695751  0.623814  0.589453\n",
      "                              base       0.701733  0.656869  0.615890\n",
      "                              component  0.559612  0.543368  0.464006\n",
      "cho2017        chan      no   adapt      0.584918  0.593246  0.600299\n",
      "                              base       0.545760  0.569485  0.606248\n",
      "                              component  0.551447  0.573710  0.578826\n",
      "                         temp adapt      0.571456  0.601975  0.612926\n",
      "                              base       0.571218  0.581492  0.613723\n",
      "                              component  0.550426  0.565336  0.565002\n",
      "               no        no   adapt      0.576410  0.597255  0.602632\n",
      "                              base       0.555691  0.577114  0.622577\n",
      "                              component  0.568393  0.562402  0.570315\n",
      "                         temp adapt      0.574712  0.578213  0.619310\n",
      "                              base       0.557125  0.576179  0.619376\n",
      "                              component  0.550554  0.566214  0.554727\n",
      "physionet      chan      no   adapt      0.373815  0.373667  0.389445\n",
      "                              base       0.362324  0.345133  0.356802\n",
      "                              component  0.391303  0.388038  0.393862\n",
      "                         temp adapt      0.378855  0.377321  0.403491\n",
      "                              base       0.357302  0.352102  0.373177\n",
      "                              component  0.382068  0.371168  0.398717\n",
      "               no        no   adapt      0.390803  0.376428  0.385886\n",
      "                              base       0.343644  0.347291  0.374298\n",
      "                              component  0.383004  0.401474  0.418930\n",
      "                         temp adapt      0.392486  0.378538  0.398788\n",
      "                              base       0.368684  0.351163  0.365788\n",
      "                              component  0.384084  0.385172  0.413875\n",
      "                                         test_acc                    \n",
      "increment_fold                                  1         2         3\n",
      "target_dataset normalize aug  model                                  \n",
      "BCI_IV         chan      no   adapt      0.713903  0.638047  0.637376\n",
      "                              base       0.669864  0.642636  0.614171\n",
      "                              component  0.578177  0.558684  0.455858\n",
      "                         temp adapt      0.742574  0.666615  0.658313\n",
      "                              base       0.738036  0.700392  0.663504\n",
      "                              component  0.658313  0.548577  0.514783\n",
      "               no        no   adapt      0.703383  0.654806  0.639198\n",
      "                              base       0.639748  0.605972  0.576423\n",
      "                              component  0.696988  0.619792  0.560197\n",
      "                         temp adapt      0.716172  0.652279  0.592203\n",
      "                              base       0.706374  0.658571  0.630776\n",
      "                              component  0.645111  0.612005  0.562362\n",
      "cho2017        chan      no   adapt      0.598402  0.605049  0.616988\n",
      "                              base       0.568306  0.583019  0.622503\n",
      "                              component  0.569414  0.588406  0.608865\n",
      "                         temp adapt      0.595884  0.608929  0.622097\n",
      "                              base       0.580884  0.584748  0.633856\n",
      "                              component  0.578750  0.591126  0.598916\n",
      "               no        no   adapt      0.606154  0.612588  0.627595\n",
      "                              base       0.571429  0.599747  0.632353\n",
      "                              component  0.587734  0.566615  0.583446\n",
      "                         temp adapt      0.602212  0.607996  0.639525\n",
      "                              base       0.572720  0.588641  0.633930\n",
      "                              component  0.582935  0.559301  0.571774\n",
      "physionet      chan      no   adapt      0.387186  0.370533  0.389173\n",
      "                              base       0.350428  0.331564  0.356297\n",
      "                              component  0.373976  0.378921  0.408690\n",
      "                         temp adapt      0.388159  0.358468  0.398053\n",
      "                              base       0.349701  0.337049  0.344425\n",
      "                              component  0.380383  0.386785  0.412997\n",
      "               no        no   adapt      0.382000  0.370636  0.393685\n",
      "                              base       0.342633  0.341669  0.345738\n",
      "                              component  0.371561  0.370453  0.399970\n",
      "                         temp adapt      0.385574  0.367401  0.381754\n",
      "                              base       0.357524  0.348546  0.360707\n",
      "                              component  0.381565  0.380032  0.395019\n"
     ]
    }
   ],
   "source": [
    "save_data_folder = \"NeurIPS\\data\"\n",
    "# save_graph_folder = \"NeurIPS\\graph\"\n",
    "group_format = data_result_1.groupby([\"normalize\",\"aug\",\"target_dataset\",\"increment_fold\",\"model\"],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['increment_fold'])\n",
    "print(table)\n",
    "output_path = os.path.join(save_data_folder,'experiment_1.xlsx')\n",
    "table.to_excel(output_path,float_format=\"%.2f\")\n",
    "group_format = data_result_2.groupby([\"normalize\",\"aug\",\"target_dataset\",\"increment_fold\",\"model\"],as_index=False).mean()\n",
    "table = pd.pivot_table(group_format, values=['test_acc'], index=['target_dataset','normalize','aug','model'],columns=['increment_fold'])\n",
    "print(table)\n",
    "output_path = os.path.join(save_data_folder,'experiment_2.xlsx')\n",
    "table.to_excel(output_path,float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee3b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
